{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm.tqdm(texts):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def build():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--corpus_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-o\", \"--output_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-s\", \"--vocab_size\", type=int, default=None)\n",
    "    parser.add_argument(\"-e\", \"--encoding\", type=str, default=\"utf-8\")\n",
    "    parser.add_argument(\"-m\", \"--min_freq\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.corpus_path, \"r\", encoding=args.encoding) as f:\n",
    "        vocab = WordVocab(f, max_size=args.vocab_size, min_freq=args.min_freq)\n",
    "\n",
    "    print(\"VOCAB SIZE:\", len(vocab))\n",
    "    vocab.save_vocab(args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "            if self.corpus_lines is None and not on_memory:\n",
    "                for _ in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                    self.corpus_lines += 1\n",
    "\n",
    "            if on_memory:\n",
    "                self.lines = [line[:-1].split(\"\\t\")\n",
    "                              for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)]\n",
    "                self.corpus_lines = len(self.lines)\n",
    "\n",
    "        if not on_memory:\n",
    "            self.file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            self.random_file = open(corpus_path, \"r\", encoding=encoding)\n",
    "\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        t1 = self.random_sent(item)\n",
    "\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n",
    "\n",
    "        t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n",
    "\n",
    "        segment_label = [1 for _ in range(len(t1))][:self.seq_len]\n",
    "    \n",
    "        bert_input = t1[:self.seq_len]\n",
    "        bert_label = t1_label[:self.seq_len]\n",
    "\n",
    "        padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  }\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()        \n",
    "        n = len(tokens)\n",
    "        ix = np.random.randint(n)\n",
    "        tokens[ix] = self.vocab.mask_index\n",
    "        output_label = [0]*n\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i!=ix:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "            else:\n",
    "                output_label[ix] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "        return tokens, output_label\n",
    "\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        t1 = self.get_corpus_line(index)\n",
    "        return t1\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        if self.on_memory:\n",
    "            return self.lines[item][0]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        if self.on_memory:\n",
    "            return self.lines[random.randrange(len(self.lines))][1]\n",
    "\n",
    "        line = self.file.__next__()\n",
    "        if line is None:\n",
    "            self.file.close()\n",
    "            self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "            line = self.random_file.__next__()\n",
    "        return line[:-1].split(\"\\t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 9961it [00:00, 331057.54it/s]\n"
     ]
    }
   ],
   "source": [
    "data = BERTDataset(corpus_path=\"wikitext-2/dataset/test_data_single_sentence.txt\",\n",
    "                  vocab=pkl.load(open(\"wikitext-2/dataset/vocab.pkl\", \"rb\")),\n",
    "                   seq_len=20,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(data, batch_size=1, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=pkl.load(open(\"wikitext-2/dataset/vocab.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bert_input': tensor([[   3,  672,    1,   26,   33,  364,  112,    4,  488,    9, 3053, 1639,\n",
      "            7,    2,    0,    0,    0,    0,    0,    0]]), 'bert_label': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_data_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
