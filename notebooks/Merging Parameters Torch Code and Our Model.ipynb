{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6920291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from models.bert import BERT \n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3524e539",
   "metadata": {},
   "source": [
    "### Load Bert-Base-Uncased Weights dumped as a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cdbd3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pretrained_model = torch.load(\"../../../torch_dump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1150f4",
   "metadata": {},
   "source": [
    "### Instantiate a dummy class to call our BERT Model to replace the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a20e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conf():\n",
    "    def __init__(self):\n",
    "        self.hidden_features = 768\n",
    "        self.layers = 12\n",
    "        self.heads = 12\n",
    "        self.device = 'cpu'\n",
    "        self.dropout = 0.1\n",
    "        self.lr = 1e-4\n",
    "        self.adam_beta1=0.999\n",
    "        self.adam_beta2 =0.999\n",
    "        self.adam_weight_decay = 1e-5\n",
    "        self.warmup_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482011c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Conf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7879784",
   "metadata": {},
   "source": [
    "### Instantiate our BERT Model \n",
    "- Ideally want for BERTLM but the same procedure will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c196900",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT(config, vocab_size=30522)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a58d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size_default = []\n",
    "for key, value in bert_pretrained_model.items():\n",
    "    param_size_default.append(value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a0d5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_size = []\n",
    "for param in bert_model.parameters():\n",
    "    param_size.append(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f0d981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([1, 512]),\n",
       " torch.Size([30522, 768]),\n",
       " torch.Size([512, 768]),\n",
       " torch.Size([2, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([30522]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([30522, 768]),\n",
       " torch.Size([30522]),\n",
       " torch.Size([2, 768]),\n",
       " torch.Size([2])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_size_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ff608c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([30522, 768]),\n",
       " torch.Size([3, 768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768, 768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([3072, 768]),\n",
       " torch.Size([3072]),\n",
       " torch.Size([768, 3072]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768]),\n",
       " torch.Size([768])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22048404",
   "metadata": {},
   "source": [
    "We see some minor disagreements especially with regard to initial embeddings. We will ignore it for the time being and proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b549425",
   "metadata": {},
   "source": [
    " Observe that the weights are stored Layerwise for the transformer modules\n",
    " \n",
    " We will just swap these weights around with the weights of our network at the correct locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b55fb3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.position_ids\n",
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "cls.predictions.bias\n",
      "cls.predictions.transform.dense.weight\n",
      "cls.predictions.transform.dense.bias\n",
      "cls.predictions.transform.LayerNorm.weight\n",
      "cls.predictions.transform.LayerNorm.bias\n",
      "cls.predictions.decoder.weight\n",
      "cls.predictions.decoder.bias\n",
      "cls.seq_relationship.weight\n",
      "cls.seq_relationship.bias\n"
     ]
    }
   ],
   "source": [
    "for key in bert_pretrained_model:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c355002",
   "metadata": {},
   "source": [
    "### Our Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8559fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (embedding): BERTEmbedding(\n",
       "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
       "    (position): PositionalEmbedding()\n",
       "    (segment): SegmentEmbedding(3, 768, padding_idx=0)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (attention): MultiHeadedAttention(\n",
       "        (linear_layers): ModuleList(\n",
       "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (attention): Attention()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (feed_forward): PositionwiseFeedForward(\n",
       "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "      )\n",
       "      (input_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output_sublayer): SublayerConnection(\n",
       "        (norm): LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02735a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.token.weight\n",
      "embedding.segment.weight\n",
      "transformer_blocks.0.attention.linear_layers.0.weight\n",
      "transformer_blocks.0.attention.linear_layers.0.bias\n",
      "transformer_blocks.0.attention.linear_layers.1.weight\n",
      "transformer_blocks.0.attention.linear_layers.1.bias\n",
      "transformer_blocks.0.attention.linear_layers.2.weight\n",
      "transformer_blocks.0.attention.linear_layers.2.bias\n",
      "transformer_blocks.0.attention.output_linear.weight\n",
      "transformer_blocks.0.attention.output_linear.bias\n",
      "transformer_blocks.0.feed_forward.w_1.weight\n",
      "transformer_blocks.0.feed_forward.w_1.bias\n",
      "transformer_blocks.0.feed_forward.w_2.weight\n",
      "transformer_blocks.0.feed_forward.w_2.bias\n",
      "transformer_blocks.0.input_sublayer.norm.a_2\n",
      "transformer_blocks.0.input_sublayer.norm.b_2\n",
      "transformer_blocks.0.output_sublayer.norm.a_2\n",
      "transformer_blocks.0.output_sublayer.norm.b_2\n",
      "transformer_blocks.1.attention.linear_layers.0.weight\n",
      "transformer_blocks.1.attention.linear_layers.0.bias\n",
      "transformer_blocks.1.attention.linear_layers.1.weight\n",
      "transformer_blocks.1.attention.linear_layers.1.bias\n",
      "transformer_blocks.1.attention.linear_layers.2.weight\n",
      "transformer_blocks.1.attention.linear_layers.2.bias\n",
      "transformer_blocks.1.attention.output_linear.weight\n",
      "transformer_blocks.1.attention.output_linear.bias\n",
      "transformer_blocks.1.feed_forward.w_1.weight\n",
      "transformer_blocks.1.feed_forward.w_1.bias\n",
      "transformer_blocks.1.feed_forward.w_2.weight\n",
      "transformer_blocks.1.feed_forward.w_2.bias\n",
      "transformer_blocks.1.input_sublayer.norm.a_2\n",
      "transformer_blocks.1.input_sublayer.norm.b_2\n",
      "transformer_blocks.1.output_sublayer.norm.a_2\n",
      "transformer_blocks.1.output_sublayer.norm.b_2\n",
      "transformer_blocks.2.attention.linear_layers.0.weight\n",
      "transformer_blocks.2.attention.linear_layers.0.bias\n",
      "transformer_blocks.2.attention.linear_layers.1.weight\n",
      "transformer_blocks.2.attention.linear_layers.1.bias\n",
      "transformer_blocks.2.attention.linear_layers.2.weight\n",
      "transformer_blocks.2.attention.linear_layers.2.bias\n",
      "transformer_blocks.2.attention.output_linear.weight\n",
      "transformer_blocks.2.attention.output_linear.bias\n",
      "transformer_blocks.2.feed_forward.w_1.weight\n",
      "transformer_blocks.2.feed_forward.w_1.bias\n",
      "transformer_blocks.2.feed_forward.w_2.weight\n",
      "transformer_blocks.2.feed_forward.w_2.bias\n",
      "transformer_blocks.2.input_sublayer.norm.a_2\n",
      "transformer_blocks.2.input_sublayer.norm.b_2\n",
      "transformer_blocks.2.output_sublayer.norm.a_2\n",
      "transformer_blocks.2.output_sublayer.norm.b_2\n",
      "transformer_blocks.3.attention.linear_layers.0.weight\n",
      "transformer_blocks.3.attention.linear_layers.0.bias\n",
      "transformer_blocks.3.attention.linear_layers.1.weight\n",
      "transformer_blocks.3.attention.linear_layers.1.bias\n",
      "transformer_blocks.3.attention.linear_layers.2.weight\n",
      "transformer_blocks.3.attention.linear_layers.2.bias\n",
      "transformer_blocks.3.attention.output_linear.weight\n",
      "transformer_blocks.3.attention.output_linear.bias\n",
      "transformer_blocks.3.feed_forward.w_1.weight\n",
      "transformer_blocks.3.feed_forward.w_1.bias\n",
      "transformer_blocks.3.feed_forward.w_2.weight\n",
      "transformer_blocks.3.feed_forward.w_2.bias\n",
      "transformer_blocks.3.input_sublayer.norm.a_2\n",
      "transformer_blocks.3.input_sublayer.norm.b_2\n",
      "transformer_blocks.3.output_sublayer.norm.a_2\n",
      "transformer_blocks.3.output_sublayer.norm.b_2\n",
      "transformer_blocks.4.attention.linear_layers.0.weight\n",
      "transformer_blocks.4.attention.linear_layers.0.bias\n",
      "transformer_blocks.4.attention.linear_layers.1.weight\n",
      "transformer_blocks.4.attention.linear_layers.1.bias\n",
      "transformer_blocks.4.attention.linear_layers.2.weight\n",
      "transformer_blocks.4.attention.linear_layers.2.bias\n",
      "transformer_blocks.4.attention.output_linear.weight\n",
      "transformer_blocks.4.attention.output_linear.bias\n",
      "transformer_blocks.4.feed_forward.w_1.weight\n",
      "transformer_blocks.4.feed_forward.w_1.bias\n",
      "transformer_blocks.4.feed_forward.w_2.weight\n",
      "transformer_blocks.4.feed_forward.w_2.bias\n",
      "transformer_blocks.4.input_sublayer.norm.a_2\n",
      "transformer_blocks.4.input_sublayer.norm.b_2\n",
      "transformer_blocks.4.output_sublayer.norm.a_2\n",
      "transformer_blocks.4.output_sublayer.norm.b_2\n",
      "transformer_blocks.5.attention.linear_layers.0.weight\n",
      "transformer_blocks.5.attention.linear_layers.0.bias\n",
      "transformer_blocks.5.attention.linear_layers.1.weight\n",
      "transformer_blocks.5.attention.linear_layers.1.bias\n",
      "transformer_blocks.5.attention.linear_layers.2.weight\n",
      "transformer_blocks.5.attention.linear_layers.2.bias\n",
      "transformer_blocks.5.attention.output_linear.weight\n",
      "transformer_blocks.5.attention.output_linear.bias\n",
      "transformer_blocks.5.feed_forward.w_1.weight\n",
      "transformer_blocks.5.feed_forward.w_1.bias\n",
      "transformer_blocks.5.feed_forward.w_2.weight\n",
      "transformer_blocks.5.feed_forward.w_2.bias\n",
      "transformer_blocks.5.input_sublayer.norm.a_2\n",
      "transformer_blocks.5.input_sublayer.norm.b_2\n",
      "transformer_blocks.5.output_sublayer.norm.a_2\n",
      "transformer_blocks.5.output_sublayer.norm.b_2\n",
      "transformer_blocks.6.attention.linear_layers.0.weight\n",
      "transformer_blocks.6.attention.linear_layers.0.bias\n",
      "transformer_blocks.6.attention.linear_layers.1.weight\n",
      "transformer_blocks.6.attention.linear_layers.1.bias\n",
      "transformer_blocks.6.attention.linear_layers.2.weight\n",
      "transformer_blocks.6.attention.linear_layers.2.bias\n",
      "transformer_blocks.6.attention.output_linear.weight\n",
      "transformer_blocks.6.attention.output_linear.bias\n",
      "transformer_blocks.6.feed_forward.w_1.weight\n",
      "transformer_blocks.6.feed_forward.w_1.bias\n",
      "transformer_blocks.6.feed_forward.w_2.weight\n",
      "transformer_blocks.6.feed_forward.w_2.bias\n",
      "transformer_blocks.6.input_sublayer.norm.a_2\n",
      "transformer_blocks.6.input_sublayer.norm.b_2\n",
      "transformer_blocks.6.output_sublayer.norm.a_2\n",
      "transformer_blocks.6.output_sublayer.norm.b_2\n",
      "transformer_blocks.7.attention.linear_layers.0.weight\n",
      "transformer_blocks.7.attention.linear_layers.0.bias\n",
      "transformer_blocks.7.attention.linear_layers.1.weight\n",
      "transformer_blocks.7.attention.linear_layers.1.bias\n",
      "transformer_blocks.7.attention.linear_layers.2.weight\n",
      "transformer_blocks.7.attention.linear_layers.2.bias\n",
      "transformer_blocks.7.attention.output_linear.weight\n",
      "transformer_blocks.7.attention.output_linear.bias\n",
      "transformer_blocks.7.feed_forward.w_1.weight\n",
      "transformer_blocks.7.feed_forward.w_1.bias\n",
      "transformer_blocks.7.feed_forward.w_2.weight\n",
      "transformer_blocks.7.feed_forward.w_2.bias\n",
      "transformer_blocks.7.input_sublayer.norm.a_2\n",
      "transformer_blocks.7.input_sublayer.norm.b_2\n",
      "transformer_blocks.7.output_sublayer.norm.a_2\n",
      "transformer_blocks.7.output_sublayer.norm.b_2\n",
      "transformer_blocks.8.attention.linear_layers.0.weight\n",
      "transformer_blocks.8.attention.linear_layers.0.bias\n",
      "transformer_blocks.8.attention.linear_layers.1.weight\n",
      "transformer_blocks.8.attention.linear_layers.1.bias\n",
      "transformer_blocks.8.attention.linear_layers.2.weight\n",
      "transformer_blocks.8.attention.linear_layers.2.bias\n",
      "transformer_blocks.8.attention.output_linear.weight\n",
      "transformer_blocks.8.attention.output_linear.bias\n",
      "transformer_blocks.8.feed_forward.w_1.weight\n",
      "transformer_blocks.8.feed_forward.w_1.bias\n",
      "transformer_blocks.8.feed_forward.w_2.weight\n",
      "transformer_blocks.8.feed_forward.w_2.bias\n",
      "transformer_blocks.8.input_sublayer.norm.a_2\n",
      "transformer_blocks.8.input_sublayer.norm.b_2\n",
      "transformer_blocks.8.output_sublayer.norm.a_2\n",
      "transformer_blocks.8.output_sublayer.norm.b_2\n",
      "transformer_blocks.9.attention.linear_layers.0.weight\n",
      "transformer_blocks.9.attention.linear_layers.0.bias\n",
      "transformer_blocks.9.attention.linear_layers.1.weight\n",
      "transformer_blocks.9.attention.linear_layers.1.bias\n",
      "transformer_blocks.9.attention.linear_layers.2.weight\n",
      "transformer_blocks.9.attention.linear_layers.2.bias\n",
      "transformer_blocks.9.attention.output_linear.weight\n",
      "transformer_blocks.9.attention.output_linear.bias\n",
      "transformer_blocks.9.feed_forward.w_1.weight\n",
      "transformer_blocks.9.feed_forward.w_1.bias\n",
      "transformer_blocks.9.feed_forward.w_2.weight\n",
      "transformer_blocks.9.feed_forward.w_2.bias\n",
      "transformer_blocks.9.input_sublayer.norm.a_2\n",
      "transformer_blocks.9.input_sublayer.norm.b_2\n",
      "transformer_blocks.9.output_sublayer.norm.a_2\n",
      "transformer_blocks.9.output_sublayer.norm.b_2\n",
      "transformer_blocks.10.attention.linear_layers.0.weight\n",
      "transformer_blocks.10.attention.linear_layers.0.bias\n",
      "transformer_blocks.10.attention.linear_layers.1.weight\n",
      "transformer_blocks.10.attention.linear_layers.1.bias\n",
      "transformer_blocks.10.attention.linear_layers.2.weight\n",
      "transformer_blocks.10.attention.linear_layers.2.bias\n",
      "transformer_blocks.10.attention.output_linear.weight\n",
      "transformer_blocks.10.attention.output_linear.bias\n",
      "transformer_blocks.10.feed_forward.w_1.weight\n",
      "transformer_blocks.10.feed_forward.w_1.bias\n",
      "transformer_blocks.10.feed_forward.w_2.weight\n",
      "transformer_blocks.10.feed_forward.w_2.bias\n",
      "transformer_blocks.10.input_sublayer.norm.a_2\n",
      "transformer_blocks.10.input_sublayer.norm.b_2\n",
      "transformer_blocks.10.output_sublayer.norm.a_2\n",
      "transformer_blocks.10.output_sublayer.norm.b_2\n",
      "transformer_blocks.11.attention.linear_layers.0.weight\n",
      "transformer_blocks.11.attention.linear_layers.0.bias\n",
      "transformer_blocks.11.attention.linear_layers.1.weight\n",
      "transformer_blocks.11.attention.linear_layers.1.bias\n",
      "transformer_blocks.11.attention.linear_layers.2.weight\n",
      "transformer_blocks.11.attention.linear_layers.2.bias\n",
      "transformer_blocks.11.attention.output_linear.weight\n",
      "transformer_blocks.11.attention.output_linear.bias\n",
      "transformer_blocks.11.feed_forward.w_1.weight\n",
      "transformer_blocks.11.feed_forward.w_1.bias\n",
      "transformer_blocks.11.feed_forward.w_2.weight\n",
      "transformer_blocks.11.feed_forward.w_2.bias\n",
      "transformer_blocks.11.input_sublayer.norm.a_2\n",
      "transformer_blocks.11.input_sublayer.norm.b_2\n",
      "transformer_blocks.11.output_sublayer.norm.a_2\n",
      "transformer_blocks.11.output_sublayer.norm.b_2\n"
     ]
    }
   ],
   "source": [
    "for name, params in bert_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53498f",
   "metadata": {},
   "source": [
    "Notice that Transformer Block Weights are what we will swap around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009407e",
   "metadata": {},
   "source": [
    "Manual Mapping Done by Me after looking closely into our Transformer Code and comparing it with the Pre-Trained Key Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85fb9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'attention.self.query.weight':'attention.linear_layers.0.weight',\n",
    "    'attention.self.query.bias':'attention.linear_layers.0.bias',\n",
    "    'attention.self.key.weight':'attention.linear_layers.1.weight',\n",
    "    'attention.self.key.bias':'attention.linear_layers.1.bias',\n",
    "    'attention.self.value.weight':'attention.linear_layers.2.weight',\n",
    "    'attention.self.value.bias':'attention.linear_layers.2.bias',\n",
    "    'attention.output.dense.weight':'attention.output_linear.weight',\n",
    "    'attention.output.dense.bias':'attention.output_linear.bias',\n",
    "    'attention.output.LayerNorm.weight':'input_sublayer.norm.a_2',\n",
    "    'attention.output.LayerNorm.bias': 'input_sublayer.norm.b_2',\n",
    "    'intermediate.dense.weight':'feed_forward.w_1.weight',\n",
    "    'intermediate.dense.bias':'feed_forward.w_1.bias',\n",
    "    'output.dense.weight':'feed_forward.w_2.weight',\n",
    "    'output.dense.bias':'feed_forward.w_2.bias',\n",
    "    'output.LayerNorm.weight':'output_sublayer.norm.a_2',\n",
    "    'output.LayerNorm.bias':'output_sublayer.norm.b_2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54c7052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping = {}\n",
    "for key, value in mapping.items():\n",
    "    inv_mapping[value] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244ae98e",
   "metadata": {},
   "source": [
    "### Set the State Dictionary\n",
    "\n",
    "- To replace weights, we will create a new state-dictionary with the BERT weights and then load that State Dictionary for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9642a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = deepcopy(bert_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f97a3f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 0, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.0.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 0, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.0.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 0, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.0.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 0, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 0, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.0.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 1, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.1.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 1, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.1.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 1, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.1.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 1, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 1, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.1.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 2, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.2.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 2, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.2.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 2, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.2.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 2, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 2, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.2.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 3, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.3.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 3, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.3.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 3, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.3.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 3, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 3, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.3.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 4, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.4.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 4, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.4.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 4, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.4.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 4, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 4, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.4.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 5, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.5.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 5, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.5.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 5, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.5.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 5, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 5, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.5.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 6, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.6.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 6, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.6.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 6, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.6.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 6, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 6, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.6.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 7, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.7.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 7, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.7.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 7, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.7.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 7, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 7, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.7.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 8, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.8.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 8, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.8.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 8, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.8.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 8, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 8, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.8.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 9, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.9.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 9, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.9.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 9, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.9.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 9, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 9, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.9.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 10, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.10.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 10, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.10.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 10, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.10.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 10, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 10, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.10.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.0.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.query.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.0.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.query.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.1.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.key.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.1.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.key.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.2.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.value.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.linear_layers.2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.self.value.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.output_linear.weight, torch.Size([768, 768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.output.dense.weight,  torch.Size([768, 768]) \n",
      "\n",
      "\n",
      "Layer: 11, attention.output_linear.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, feed_forward.w_1.weight, torch.Size([3072, 768]), \n",
      "\t\t\t bert.encoder.layer.11.intermediate.dense.weight,  torch.Size([3072, 768]) \n",
      "\n",
      "\n",
      "Layer: 11, feed_forward.w_1.bias, torch.Size([3072]), \n",
      "\t\t\t bert.encoder.layer.11.intermediate.dense.bias,  torch.Size([3072]) \n",
      "\n",
      "\n",
      "Layer: 11, feed_forward.w_2.weight, torch.Size([768, 3072]), \n",
      "\t\t\t bert.encoder.layer.11.output.dense.weight,  torch.Size([768, 3072]) \n",
      "\n",
      "\n",
      "Layer: 11, feed_forward.w_2.bias, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.output.dense.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, input_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, input_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.attention.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, output_sublayer.norm.a_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.output.LayerNorm.weight,  torch.Size([768]) \n",
      "\n",
      "\n",
      "Layer: 11, output_sublayer.norm.b_2, torch.Size([768]), \n",
      "\t\t\t bert.encoder.layer.11.output.LayerNorm.bias,  torch.Size([768]) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for layer in range(12):\n",
    "    # We have 12 transformer layers, iterate through them one by one\n",
    "    for name, p_val in bert_model.transformer_blocks[layer].named_parameters():\n",
    "        # Iterate through each transformer back one by one, name is name of the parameter (refer to mapping)\n",
    "        # p_val is the value of the parameter --> We want to change this value :)\n",
    "        to_copy = f'bert.encoder.layer.{layer}.' + inv_mapping[name]\n",
    "        # to_copy is the name of the same parameter in the pre-trained BERT model, obtained by invert_map \n",
    "        # refer to inv_mapping above obtained by swapping keys and values of mapping\n",
    "        # I first created mapping but later realized we needed inverse mapping and not mapping per say\n",
    "        param_to_copy = bert_pretrained_model[to_copy]\n",
    "        # Obtain the parameter to copy by indexing into the Dictionary that stores the weights from PT BERT\n",
    "        dic[f'transformer_blocks.{layer}.' + name] = param_to_copy\n",
    "        # Set the value of the parameter in this state dictionary\n",
    "        assert p_val.shape == param_to_copy.shape\n",
    "        print(f\"Layer: {layer}, {name}, {p_val.shape}, \\n\\t\\t\\t {to_copy},  {param_to_copy.shape} \\n\\n\")\n",
    "        # Log \n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01e8367",
   "metadata": {},
   "source": [
    "### Obtain New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcd310c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09fc4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_model, \"./BERT_with_PT_weights.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc70fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = bert_model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
