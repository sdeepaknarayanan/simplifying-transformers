{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import datasets\n",
    "from models import BERTLM\n",
    "from models.merged_retrained_bert import MergedRetrainedBert\n",
    "from models.merged_retrained_bert import RetrainedBlock\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab = \"bert-google\"\n",
    "        self.vocab_path = \"../data/wikitext2/all.txt\"\n",
    "        self.bert_google_vocab = \"../data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.test_dataset = \"../data/wikitext2/test_data_single_sentence.txt\"\n",
    "        self.vocab_max_size = None\n",
    "        self.vocab_min_frequency = 1\n",
    "        self.dataset = \"wikitext2\"\n",
    "        self.seq_len = 40\n",
    "        self.on_memory = True\n",
    "        self.corpus_lines = None\n",
    "        self.train_dataset = \"../data/wikitext2/test_data_single_sentence.txt\"\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1\n",
    "        self.hidden_features = 768\n",
    "        self.layers = 12\n",
    "        self.heads = 12\n",
    "        self.device = \"cpu\"\n",
    "        self.dropout = 0.1\n",
    "        self.train = True\n",
    "        self.lr = 1e-3\n",
    "        self.adam_beta1=0.999\n",
    "        self.adam_beta2=0.999\n",
    "        self.adam_weight_decay = 0.01\n",
    "        self.warmup_steps =1000\n",
    "        self.storage_directory = \"C:/Users/Raphi/PycharmProjects/simplifying-transformers\"\n",
    "        self.model = \"MergedRetrainedBert\"\n",
    "        self.model_checkpoint = \"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "config = config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:00, 122650.20it/s]\n",
      "30522it [00:00, 1017239.28it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = datasets.get_vocab(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from [../models/_checkpoints/wikitext2/BERTLM-latest.pth]\n",
      "Loaded checkpoint\n",
      "Successfully loaded state dict\n"
     ]
    },
    {
     "data": {
      "text/plain": "BERTLM(\n  (bert): BERT(\n    (embedding): BERTEmbedding(\n      (token): TokenEmbedding(30522, 768, padding_idx=0)\n      (position): PositionalEmbedding()\n      (segment): SegmentEmbedding(2, 768, padding_idx=0)\n      (dropout): Dropout(p=0.1, inplace=False)\n      (layer_norm): LayerNorm()\n    )\n    (transformer_blocks): ModuleList(\n      (0): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (1): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (2): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (3): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (4): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (5): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (6): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (7): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (8): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (9): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (10): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (11): TransformerBlock(\n        (attention): MultiHeadedAttention(\n          (linear_layers): ModuleList(\n            (0): Linear(in_features=768, out_features=768, bias=True)\n            (1): Linear(in_features=768, out_features=768, bias=True)\n            (2): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n          (attention): Attention()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): PositionwiseFeedForward(\n          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation): GELU(approximate='none')\n        )\n        (input_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (output_sublayer): SublayerConnection(\n          (norm): LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n  )\n  (mask_lm): MaskedLanguageModel(\n    (linear): Linear(in_features=768, out_features=768, bias=True)\n    (act): GELU(approximate='none')\n    (layer_norm): LayerNorm()\n    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    (softmax): LogSoftmax(dim=-1)\n  )\n)"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
    "dks = [64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64]\n",
    "merged = MergedRetrainedBert(\n",
    "    config,\n",
    "    vocab_size=len(vocab),\n",
    "    dks=dks,\n",
    "    heads=heads\n",
    ")\n",
    "\n",
    "bert = BERTLM(config, vocab_size=len(vocab))\n",
    "bert.load_state(load_optimizer=False, overwrite_path=\"../models/_checkpoints/wikitext2/BERTLM-latest.pth\")\n",
    "bert.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MergedRetrainedBert(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding()\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(merged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTLM(\n",
      "  (bert): BERT(\n",
      "    (embedding): BERTEmbedding(\n",
      "      (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "      (position): PositionalEmbedding()\n",
      "      (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm): LayerNorm()\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy Encoder from Bert to our Merged Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embedding.token.weight torch.Size([30522, 768])\n",
      "bert.embedding.segment.weight torch.Size([2, 768])\n",
      "bert.embedding.layer_norm.a_2 torch.Size([768])\n",
      "bert.embedding.layer_norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.b_2 torch.Size([768])\n",
      "mask_lm.linear.weight torch.Size([768, 768])\n",
      "mask_lm.linear.bias torch.Size([768])\n",
      "mask_lm.layer_norm.a_2 torch.Size([768])\n",
      "mask_lm.layer_norm.b_2 torch.Size([768])\n",
      "mask_lm.decoder.weight torch.Size([30522, 768])\n",
      "mask_lm.decoder.bias torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert.named_parameters():\n",
    "    print(name, param.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "merged.embedding.token.weight.data = deepcopy(bert.bert.embedding.token.weight)\n",
    "merged.embedding.position.pe = deepcopy(bert.bert.embedding.position.pe)\n",
    "merged.embedding.segment.weight.data = deepcopy(bert.bert.embedding.segment.weight)\n",
    "merged.embedding.layer_norm.a_2.data = deepcopy(bert.bert.embedding.layer_norm.a_2)\n",
    "merged.embedding.layer_norm.b_2.data = deepcopy(bert.bert.embedding.layer_norm.b_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy MLP weights from Bert and Attention weights from retrained blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_0_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_1_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_2_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_3_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_4_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_5_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_6_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_7_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_8_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_9_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_10_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_11_64_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n"
     ]
    }
   ],
   "source": [
    "# load weights for each transformer block\n",
    "for index in range(12):\n",
    "\n",
    "    retrained = RetrainedBlock(config, depth=index, hidden=config.hidden_features, heads=heads[index], dk=dks[index], dropout=config.dropout)\n",
    "    retrained.load_state(load_optimizer=False)\n",
    "\n",
    "    # for j in range(3):\n",
    "    #     merged.layers[index].attentionblock.linear_layers[j].weight.data = deepcopy(bert.bert.transformer_blocks[index].attention.linear_layers[j].weight)\n",
    "    #     merged.layers[index].attentionblock.linear_layers[j].bias.data = deepcopy(bert.bert.transformer_blocks[index].attention.linear_layers[j].bias)\n",
    "    #\n",
    "    # merged.layers[index].attentionblock.output_linear.weight.data = deepcopy(bert.bert.transformer_blocks[index].attention.output_linear.weight)\n",
    "    # merged.layers[index].attentionblock.output_linear.bias.data = deepcopy(bert.bert.transformer_blocks[index].attention.output_linear.bias)\n",
    "\n",
    "    # block_checkpoints\n",
    "    for j in range(3):\n",
    "        merged.layers[index].attentionblock.linear_layers[j].weight.data = retrained.attentionblock.linear_layers[j].weight\n",
    "        merged.layers[index].attentionblock.linear_layers[j].bias.data = retrained.attentionblock.linear_layers[j].bias\n",
    "\n",
    "    merged.layers[index].attentionblock.output_linear.weight.data = retrained.attentionblock.output_linear.weight\n",
    "    merged.layers[index].attentionblock.output_linear.bias.data = retrained.attentionblock.output_linear.bias\n",
    "\n",
    "    # feed_forward\n",
    "    merged.layers[index].feed_forward.w_1.weight.data = deepcopy(bert.bert.transformer_blocks[index].feed_forward.w_1.weight)\n",
    "    merged.layers[index].feed_forward.w_1.bias.data = deepcopy(bert.bert.transformer_blocks[index].feed_forward.w_1.bias)\n",
    "    merged.layers[index].feed_forward.w_2.weight.data = deepcopy(bert.bert.transformer_blocks[index].feed_forward.w_2.weight)\n",
    "    merged.layers[index].feed_forward.w_2.bias.data = deepcopy(bert.bert.transformer_blocks[index].feed_forward.w_2.bias)\n",
    "\n",
    "    #input_sublayer\n",
    "    merged.layers[index].input_sublayer.norm.a_2.data = deepcopy(bert.bert.transformer_blocks[index].input_sublayer.norm.a_2)\n",
    "    merged.layers[index].input_sublayer.norm.b_2.data = deepcopy(bert.bert.transformer_blocks[index].input_sublayer.norm.b_2)\n",
    "\n",
    "    #output_sublayer\n",
    "    merged.layers[index].output_sublayer.norm.a_2.data = deepcopy(bert.bert.transformer_blocks[index].output_sublayer.norm.a_2)\n",
    "    merged.layers[index].output_sublayer.norm.b_2.data = deepcopy(bert.bert.transformer_blocks[index].output_sublayer.norm.b_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy MaskLM Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "merged.mask_lm.linear.weight.data = deepcopy(bert.mask_lm.linear.weight)\n",
    "merged.mask_lm.linear.bias.data = deepcopy(bert.mask_lm.linear.bias)\n",
    "merged.mask_lm.layer_norm.a_2.data = deepcopy(bert.mask_lm.layer_norm.a_2)\n",
    "merged.mask_lm.layer_norm.b_2.data = deepcopy(bert.mask_lm.layer_norm.b_2)\n",
    "merged.mask_lm.decoder.weight.data = deepcopy(bert.mask_lm.decoder.weight)\n",
    "merged.mask_lm.decoder.bias.data = deepcopy(bert.mask_lm.decoder.bias)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-22.7625, -22.8823, -21.2407,  ..., -21.9298, -20.7592, -20.0416],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "eat\n",
      "tensor([-16.0766, -16.5733, -16.6725,  ..., -16.2520, -16.4822, -10.4133],\n",
      "       grad_fn=<SelectBackward0>)\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "sentence = [\"[CLS]\", \"i\", \"like\", \"to\", \"[MASK]\", \"pizza\", \"[SEP]\"] + [\"[PAD]\"] * 33\n",
    "segment_label = torch.from_numpy(numpy.array([1] * 7 + [0] * 33))\n",
    "ids = numpy.array(list(map(lambda x: vocab.stoi[x], sentence)))\n",
    "ids = torch.unsqueeze(torch.from_numpy(ids), dim=0)\n",
    "bert.eval()\n",
    "merged.eval()\n",
    "teacher_pred = bert(ids.to(config.device), segment_label.to(config.device))\n",
    "teacher_pred = teacher_pred[0][4]\n",
    "print(teacher_pred)\n",
    "teacher_pred = torch.argmax(teacher_pred, dim=0)\n",
    "teacher_word = vocab.itos[teacher_pred]\n",
    "print(teacher_word)\n",
    "\n",
    "pred = merged(ids.to(config.device), segment_label.to(config.device))\n",
    "pred = pred[0][4]\n",
    "print(pred)\n",
    "pred = torch.argmax(pred, dim=0)\n",
    "word = vocab.itos[pred]\n",
    "print(word)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\t embedding.token.weight\n",
      "True\t embedding.segment.weight\n",
      "True\t embedding.layer_norm.a_2\n",
      "True\t embedding.layer_norm.b_2\n",
      "False\t layers.0.attentionblock.linear_layers.0.weight\n",
      "False\t layers.0.attentionblock.linear_layers.0.bias\n",
      "False\t layers.0.attentionblock.linear_layers.1.weight\n",
      "False\t layers.0.attentionblock.linear_layers.1.bias\n",
      "False\t layers.0.attentionblock.linear_layers.2.weight\n",
      "False\t layers.0.attentionblock.linear_layers.2.bias\n",
      "False\t layers.0.attentionblock.output_linear.weight\n",
      "False\t layers.0.attentionblock.output_linear.bias\n",
      "True\t layers.0.feed_forward.w_1.weight\n",
      "True\t layers.0.feed_forward.w_1.bias\n",
      "True\t layers.0.feed_forward.w_2.weight\n",
      "True\t layers.0.feed_forward.w_2.bias\n",
      "True\t layers.0.input_sublayer.norm.a_2\n",
      "True\t layers.0.input_sublayer.norm.b_2\n",
      "True\t layers.0.output_sublayer.norm.a_2\n",
      "True\t layers.0.output_sublayer.norm.b_2\n",
      "False\t layers.1.attentionblock.linear_layers.0.weight\n",
      "False\t layers.1.attentionblock.linear_layers.0.bias\n",
      "False\t layers.1.attentionblock.linear_layers.1.weight\n",
      "False\t layers.1.attentionblock.linear_layers.1.bias\n",
      "False\t layers.1.attentionblock.linear_layers.2.weight\n",
      "False\t layers.1.attentionblock.linear_layers.2.bias\n",
      "False\t layers.1.attentionblock.output_linear.weight\n",
      "False\t layers.1.attentionblock.output_linear.bias\n",
      "True\t layers.1.feed_forward.w_1.weight\n",
      "True\t layers.1.feed_forward.w_1.bias\n",
      "True\t layers.1.feed_forward.w_2.weight\n",
      "True\t layers.1.feed_forward.w_2.bias\n",
      "True\t layers.1.input_sublayer.norm.a_2\n",
      "True\t layers.1.input_sublayer.norm.b_2\n",
      "True\t layers.1.output_sublayer.norm.a_2\n",
      "True\t layers.1.output_sublayer.norm.b_2\n",
      "False\t layers.2.attentionblock.linear_layers.0.weight\n",
      "False\t layers.2.attentionblock.linear_layers.0.bias\n",
      "False\t layers.2.attentionblock.linear_layers.1.weight\n",
      "False\t layers.2.attentionblock.linear_layers.1.bias\n",
      "False\t layers.2.attentionblock.linear_layers.2.weight\n",
      "False\t layers.2.attentionblock.linear_layers.2.bias\n",
      "False\t layers.2.attentionblock.output_linear.weight\n",
      "False\t layers.2.attentionblock.output_linear.bias\n",
      "True\t layers.2.feed_forward.w_1.weight\n",
      "True\t layers.2.feed_forward.w_1.bias\n",
      "True\t layers.2.feed_forward.w_2.weight\n",
      "True\t layers.2.feed_forward.w_2.bias\n",
      "True\t layers.2.input_sublayer.norm.a_2\n",
      "True\t layers.2.input_sublayer.norm.b_2\n",
      "True\t layers.2.output_sublayer.norm.a_2\n",
      "True\t layers.2.output_sublayer.norm.b_2\n",
      "False\t layers.3.attentionblock.linear_layers.0.weight\n",
      "False\t layers.3.attentionblock.linear_layers.0.bias\n",
      "False\t layers.3.attentionblock.linear_layers.1.weight\n",
      "False\t layers.3.attentionblock.linear_layers.1.bias\n",
      "False\t layers.3.attentionblock.linear_layers.2.weight\n",
      "False\t layers.3.attentionblock.linear_layers.2.bias\n",
      "False\t layers.3.attentionblock.output_linear.weight\n",
      "False\t layers.3.attentionblock.output_linear.bias\n",
      "True\t layers.3.feed_forward.w_1.weight\n",
      "True\t layers.3.feed_forward.w_1.bias\n",
      "True\t layers.3.feed_forward.w_2.weight\n",
      "True\t layers.3.feed_forward.w_2.bias\n",
      "True\t layers.3.input_sublayer.norm.a_2\n",
      "True\t layers.3.input_sublayer.norm.b_2\n",
      "True\t layers.3.output_sublayer.norm.a_2\n",
      "True\t layers.3.output_sublayer.norm.b_2\n",
      "False\t layers.4.attentionblock.linear_layers.0.weight\n",
      "False\t layers.4.attentionblock.linear_layers.0.bias\n",
      "False\t layers.4.attentionblock.linear_layers.1.weight\n",
      "False\t layers.4.attentionblock.linear_layers.1.bias\n",
      "False\t layers.4.attentionblock.linear_layers.2.weight\n",
      "False\t layers.4.attentionblock.linear_layers.2.bias\n",
      "False\t layers.4.attentionblock.output_linear.weight\n",
      "False\t layers.4.attentionblock.output_linear.bias\n",
      "True\t layers.4.feed_forward.w_1.weight\n",
      "True\t layers.4.feed_forward.w_1.bias\n",
      "True\t layers.4.feed_forward.w_2.weight\n",
      "True\t layers.4.feed_forward.w_2.bias\n",
      "True\t layers.4.input_sublayer.norm.a_2\n",
      "True\t layers.4.input_sublayer.norm.b_2\n",
      "True\t layers.4.output_sublayer.norm.a_2\n",
      "True\t layers.4.output_sublayer.norm.b_2\n",
      "False\t layers.5.attentionblock.linear_layers.0.weight\n",
      "False\t layers.5.attentionblock.linear_layers.0.bias\n",
      "False\t layers.5.attentionblock.linear_layers.1.weight\n",
      "False\t layers.5.attentionblock.linear_layers.1.bias\n",
      "False\t layers.5.attentionblock.linear_layers.2.weight\n",
      "False\t layers.5.attentionblock.linear_layers.2.bias\n",
      "False\t layers.5.attentionblock.output_linear.weight\n",
      "False\t layers.5.attentionblock.output_linear.bias\n",
      "True\t layers.5.feed_forward.w_1.weight\n",
      "True\t layers.5.feed_forward.w_1.bias\n",
      "True\t layers.5.feed_forward.w_2.weight\n",
      "True\t layers.5.feed_forward.w_2.bias\n",
      "True\t layers.5.input_sublayer.norm.a_2\n",
      "True\t layers.5.input_sublayer.norm.b_2\n",
      "True\t layers.5.output_sublayer.norm.a_2\n",
      "True\t layers.5.output_sublayer.norm.b_2\n",
      "False\t layers.6.attentionblock.linear_layers.0.weight\n",
      "False\t layers.6.attentionblock.linear_layers.0.bias\n",
      "False\t layers.6.attentionblock.linear_layers.1.weight\n",
      "False\t layers.6.attentionblock.linear_layers.1.bias\n",
      "False\t layers.6.attentionblock.linear_layers.2.weight\n",
      "False\t layers.6.attentionblock.linear_layers.2.bias\n",
      "False\t layers.6.attentionblock.output_linear.weight\n",
      "False\t layers.6.attentionblock.output_linear.bias\n",
      "True\t layers.6.feed_forward.w_1.weight\n",
      "True\t layers.6.feed_forward.w_1.bias\n",
      "True\t layers.6.feed_forward.w_2.weight\n",
      "True\t layers.6.feed_forward.w_2.bias\n",
      "True\t layers.6.input_sublayer.norm.a_2\n",
      "True\t layers.6.input_sublayer.norm.b_2\n",
      "True\t layers.6.output_sublayer.norm.a_2\n",
      "True\t layers.6.output_sublayer.norm.b_2\n",
      "False\t layers.7.attentionblock.linear_layers.0.weight\n",
      "False\t layers.7.attentionblock.linear_layers.0.bias\n",
      "False\t layers.7.attentionblock.linear_layers.1.weight\n",
      "False\t layers.7.attentionblock.linear_layers.1.bias\n",
      "False\t layers.7.attentionblock.linear_layers.2.weight\n",
      "False\t layers.7.attentionblock.linear_layers.2.bias\n",
      "False\t layers.7.attentionblock.output_linear.weight\n",
      "False\t layers.7.attentionblock.output_linear.bias\n",
      "True\t layers.7.feed_forward.w_1.weight\n",
      "True\t layers.7.feed_forward.w_1.bias\n",
      "True\t layers.7.feed_forward.w_2.weight\n",
      "True\t layers.7.feed_forward.w_2.bias\n",
      "True\t layers.7.input_sublayer.norm.a_2\n",
      "True\t layers.7.input_sublayer.norm.b_2\n",
      "True\t layers.7.output_sublayer.norm.a_2\n",
      "True\t layers.7.output_sublayer.norm.b_2\n",
      "False\t layers.8.attentionblock.linear_layers.0.weight\n",
      "False\t layers.8.attentionblock.linear_layers.0.bias\n",
      "False\t layers.8.attentionblock.linear_layers.1.weight\n",
      "False\t layers.8.attentionblock.linear_layers.1.bias\n",
      "False\t layers.8.attentionblock.linear_layers.2.weight\n",
      "False\t layers.8.attentionblock.linear_layers.2.bias\n",
      "False\t layers.8.attentionblock.output_linear.weight\n",
      "False\t layers.8.attentionblock.output_linear.bias\n",
      "True\t layers.8.feed_forward.w_1.weight\n",
      "True\t layers.8.feed_forward.w_1.bias\n",
      "True\t layers.8.feed_forward.w_2.weight\n",
      "True\t layers.8.feed_forward.w_2.bias\n",
      "True\t layers.8.input_sublayer.norm.a_2\n",
      "True\t layers.8.input_sublayer.norm.b_2\n",
      "True\t layers.8.output_sublayer.norm.a_2\n",
      "True\t layers.8.output_sublayer.norm.b_2\n",
      "False\t layers.9.attentionblock.linear_layers.0.weight\n",
      "False\t layers.9.attentionblock.linear_layers.0.bias\n",
      "False\t layers.9.attentionblock.linear_layers.1.weight\n",
      "False\t layers.9.attentionblock.linear_layers.1.bias\n",
      "False\t layers.9.attentionblock.linear_layers.2.weight\n",
      "False\t layers.9.attentionblock.linear_layers.2.bias\n",
      "False\t layers.9.attentionblock.output_linear.weight\n",
      "False\t layers.9.attentionblock.output_linear.bias\n",
      "True\t layers.9.feed_forward.w_1.weight\n",
      "True\t layers.9.feed_forward.w_1.bias\n",
      "True\t layers.9.feed_forward.w_2.weight\n",
      "True\t layers.9.feed_forward.w_2.bias\n",
      "True\t layers.9.input_sublayer.norm.a_2\n",
      "True\t layers.9.input_sublayer.norm.b_2\n",
      "True\t layers.9.output_sublayer.norm.a_2\n",
      "True\t layers.9.output_sublayer.norm.b_2\n",
      "False\t layers.10.attentionblock.linear_layers.0.weight\n",
      "False\t layers.10.attentionblock.linear_layers.0.bias\n",
      "False\t layers.10.attentionblock.linear_layers.1.weight\n",
      "False\t layers.10.attentionblock.linear_layers.1.bias\n",
      "False\t layers.10.attentionblock.linear_layers.2.weight\n",
      "False\t layers.10.attentionblock.linear_layers.2.bias\n",
      "False\t layers.10.attentionblock.output_linear.weight\n",
      "False\t layers.10.attentionblock.output_linear.bias\n",
      "True\t layers.10.feed_forward.w_1.weight\n",
      "True\t layers.10.feed_forward.w_1.bias\n",
      "True\t layers.10.feed_forward.w_2.weight\n",
      "True\t layers.10.feed_forward.w_2.bias\n",
      "True\t layers.10.input_sublayer.norm.a_2\n",
      "True\t layers.10.input_sublayer.norm.b_2\n",
      "True\t layers.10.output_sublayer.norm.a_2\n",
      "True\t layers.10.output_sublayer.norm.b_2\n",
      "False\t layers.11.attentionblock.linear_layers.0.weight\n",
      "False\t layers.11.attentionblock.linear_layers.0.bias\n",
      "False\t layers.11.attentionblock.linear_layers.1.weight\n",
      "False\t layers.11.attentionblock.linear_layers.1.bias\n",
      "False\t layers.11.attentionblock.linear_layers.2.weight\n",
      "False\t layers.11.attentionblock.linear_layers.2.bias\n",
      "False\t layers.11.attentionblock.output_linear.weight\n",
      "False\t layers.11.attentionblock.output_linear.bias\n",
      "True\t layers.11.feed_forward.w_1.weight\n",
      "True\t layers.11.feed_forward.w_1.bias\n",
      "True\t layers.11.feed_forward.w_2.weight\n",
      "True\t layers.11.feed_forward.w_2.bias\n",
      "True\t layers.11.input_sublayer.norm.a_2\n",
      "True\t layers.11.input_sublayer.norm.b_2\n",
      "True\t layers.11.output_sublayer.norm.a_2\n",
      "True\t layers.11.output_sublayer.norm.b_2\n",
      "True\t mask_lm.linear.weight\n",
      "True\t mask_lm.linear.bias\n",
      "True\t mask_lm.layer_norm.a_2\n",
      "True\t mask_lm.layer_norm.b_2\n",
      "True\t mask_lm.decoder.weight\n",
      "True\t mask_lm.decoder.bias\n"
     ]
    }
   ],
   "source": [
    "for (name, param), (na, pa) in zip(merged.named_parameters(), bert.named_parameters()):\n",
    "    equal = param == pa\n",
    "    equal = torch.all(equal)\n",
    "\n",
    "    print(str(equal.item()) + \"\\t\", name)\n",
    "\n",
    "    # print(name, param.size(), na, pa.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "merged.save_model(running=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}