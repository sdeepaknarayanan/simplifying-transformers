{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from models import BERTLM\n",
    "from models.squish_bert import SquishBert\n",
    "from models.squish_bert import RetrainedBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab = \"bert-google\"\n",
    "        self.vocab_path = \"../data/wikitext2/all.txt\"\n",
    "        self.bert_google_vocab = \"../data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.vocab_max_size = None\n",
    "        self.vocab_min_frequency = 1\n",
    "        self.dataset = \"wikitext2\"\n",
    "        self.seq_len = 40\n",
    "        self.on_memory = True\n",
    "        self.corpus_lines = None\n",
    "        self.train_dataset = \"../data/wikitext2/test_data_single_sentence.txt\"\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1\n",
    "        self.hidden_features = 768\n",
    "        self.layers = 12\n",
    "        self.heads = 12\n",
    "        self.device = \"cpu\"\n",
    "        self.dropout = 0.1\n",
    "        self.train = True\n",
    "        self.lr = 1e-3\n",
    "        self.adam_beta1=0.999\n",
    "        self.adam_beta2=0.999\n",
    "        self.adam_weight_decay = 0.01\n",
    "        self.warmup_steps =1000\n",
    "        self.storage_directory = \"C:/Users/Raphi/PycharmProjects/simplifying-transformers\"\n",
    "        self.model = \"SquishBert\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "config = config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:00, 114284.49it/s]\n",
      "30522it [00:00, 984621.72it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = datasets.get_vocab(config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "heads = [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
    "dks = [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]\n",
    "squished = SquishBert(\n",
    "    config,\n",
    "    vocab_size=len(vocab),\n",
    "    dks=dks,\n",
    "    heads=heads\n",
    ")\n",
    "\n",
    "bert = BERTLM(config, vocab_size=len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquishBert(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding()\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(squished)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTLM(\n",
      "  (bert): BERT(\n",
      "    (embedding): BERTEmbedding(\n",
      "      (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "      (position): PositionalEmbedding()\n",
      "      (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (layer_norm): LayerNorm()\n",
      "    )\n",
      "    (transformer_blocks): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (6): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (7): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (8): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (9): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (10): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (11): TransformerBlock(\n",
      "        (attention): MultiHeadedAttention(\n",
      "          (linear_layers): ModuleList(\n",
      "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attention): Attention()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "        (input_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (output_sublayer): SublayerConnection(\n",
      "          (norm): LayerNorm()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(bert)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy Encoder from Bert to our Merged Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "squished.embedding.token.weight.data = bert.bert.embedding.token.weight\n",
    "squished.embedding.segment.weight.data = bert.bert.embedding.segment.weight\n",
    "squished.embedding.layer_norm.a_2.data = bert.bert.embedding.layer_norm.a_2\n",
    "squished.embedding.layer_norm.b_2.data = bert.bert.embedding.layer_norm.b_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy MLP weights from Bert and Attention weights from retrained blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_0_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_1_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_2_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_3_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_4_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_5_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_6_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_7_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_8_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_9_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_10_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n",
      "Block Path: C:/Users/Raphi/PycharmProjects/simplifying-transformers/models/_checkpoints/wikitext2/block_11_16_12/BLOCK-latest.pth\n",
      "Loaded block checkpoint\n",
      "Successfully loaded state dict for block model\n"
     ]
    }
   ],
   "source": [
    "# load weights for each transformer block\n",
    "for index in range(12):\n",
    "    retrained = RetrainedBlock(config, depth=index, hidden=config.hidden_features, heads=heads[index], dk=dks[index], dropout=config.dropout)\n",
    "    retrained.load_state(load_optimizer=False)\n",
    "\n",
    "    # block_checkpoints\n",
    "    for j in range(3):\n",
    "        squished.layers[index].attentionblock.linear_layers[j].weight.data = retrained.attentionblock.linear_layers[j].weight\n",
    "        squished.layers[index].attentionblock.linear_layers[j].bias.data = retrained.attentionblock.linear_layers[j].bias\n",
    "\n",
    "    squished.layers[index].attentionblock.output_linear.weight.data = retrained.attentionblock.output_linear.weight\n",
    "    squished.layers[index].attentionblock.output_linear.bias.data = retrained.attentionblock.output_linear.bias\n",
    "\n",
    "    # feed_forward\n",
    "    squished.layers[index].feed_forward.w_1.weight.data = bert.bert.transformer_blocks[index].feed_forward.w_1.weight\n",
    "    squished.layers[index].feed_forward.w_1.bias.data = bert.bert.transformer_blocks[index].feed_forward.w_1.bias\n",
    "    squished.layers[index].feed_forward.w_2.weight.data = bert.bert.transformer_blocks[index].feed_forward.w_2.weight\n",
    "    squished.layers[index].feed_forward.w_2.bias.data = bert.bert.transformer_blocks[index].feed_forward.w_2.bias\n",
    "\n",
    "    #input_sublayer\n",
    "    squished.layers[index].input_sublayer.norm.a_2.data = bert.bert.transformer_blocks[index].input_sublayer.norm.a_2\n",
    "    squished.layers[index].input_sublayer.norm.b_2.data = bert.bert.transformer_blocks[index].input_sublayer.norm.b_2\n",
    "\n",
    "    #output_sublayer\n",
    "    squished.layers[index].output_sublayer.norm.a_2.data = bert.bert.transformer_blocks[index].output_sublayer.norm.a_2\n",
    "    squished.layers[index].output_sublayer.norm.b_2.data = bert.bert.transformer_blocks[index].output_sublayer.norm.b_2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy MaskLM Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "squished.mask_lm.linear.weight.data = bert.mask_lm.linear.weight\n",
    "squished.mask_lm.linear.bias.data = bert.mask_lm.linear.bias\n",
    "squished.mask_lm.layer_norm.a_2.data = bert.mask_lm.layer_norm.a_2\n",
    "squished.mask_lm.layer_norm.b_2.data = bert.mask_lm.layer_norm.b_2\n",
    "squished.mask_lm.decoder.weight.data = bert.mask_lm.decoder.weight\n",
    "squished.mask_lm.decoder.bias.data = bert.mask_lm.decoder.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SquishBert(\n",
      "  (embedding): BERTEmbedding(\n",
      "    (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
      "    (position): PositionalEmbedding()\n",
      "    (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (layers): Sequential(\n",
      "    (0): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (6): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (7): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (9): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (11): RetrainedTransformer(\n",
      "      (attentionblock): BlockMultiHeadedAttention(\n",
      "        (linear_layers): ModuleList(\n",
      "          (0): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (1): Linear(in_features=768, out_features=192, bias=True)\n",
      "          (2): Linear(in_features=768, out_features=192, bias=True)\n",
      "        )\n",
      "        (output_linear): Linear(in_features=192, out_features=768, bias=True)\n",
      "        (attention): Attention()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (feed_forward): PositionwiseFeedForward(\n",
      "        (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (input_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (output_sublayer): SublayerConnection(\n",
      "        (norm): LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (mask_lm): MaskedLanguageModel(\n",
      "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (layer_norm): LayerNorm()\n",
      "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
      "    (softmax): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Cope\n",
    "for i in range(1000):\n",
    "    # scream\n",
    "    continue\n",
    "print(squished)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "squished.save_model(running=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}