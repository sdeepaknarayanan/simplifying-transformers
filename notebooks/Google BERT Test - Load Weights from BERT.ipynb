{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c563a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e9a5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.vocabulary import BertVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba92cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab_path = \"../data/uncased_L-12_H-768_A-12/vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4f84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36ba6b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a0a1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5c94a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_counter = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54676de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:01, 80945.86it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Bert Vocab\")\n",
    "counter = Counter()\n",
    "with open(\"../data/wikitext2/all.txt\") as text_file:\n",
    "    for line in tqdm.tqdm(text_file):\n",
    "        if isinstance(line, list):\n",
    "            words = line\n",
    "        else:\n",
    "            words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "        for word in words:\n",
    "            counter[word] += 1\n",
    "\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8688100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "573ad8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30522it [00:00, 184377.62it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_counter = Counter()\n",
    "word_list = []\n",
    "with open(\"../data/uncased_L-12_H-768_A-12/vocab.txt\") as bert_vocab:\n",
    "\n",
    "    for word in tqdm.tqdm(bert_vocab):\n",
    "        word = word.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "        bert_counter[word] = counter[word] if word in counter else 1\n",
    "        word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2026414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71972223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab = \"bert-google\"\n",
    "        self.vocab_path = \"../data/wikitext2/all.txt\"\n",
    "        self.bert_google_vocab = \"../data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.vocab_max_size = None\n",
    "        self.vocab_min_frequency = 1\n",
    "        self.dataset = \"wikitext2\"\n",
    "        self.seq_len = 40\n",
    "        self.on_memory = True\n",
    "        self.corpus_lines = None\n",
    "        self.train_dataset = \"../data/wikitext2/test_data_single_sentence.txt\"\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1c1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c36981b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:00, 121759.57it/s]\n",
      "30522it [00:00, 1140365.28it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = BertVocab(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0933c786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 9961it [00:00, 15294.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# load the dataset specified with --dataset_name & get data loaders\n",
    "train_dataset = datasets.get(dataset_name=\"wikitext2\")(config=conf, vocab=vocab)\n",
    "\n",
    "train_loader = train_dataset.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb37a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787153f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e122a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.hidden_features = 768\n",
    "conf.layers = 12\n",
    "conf.heads = 12\n",
    "conf.device = \"cpu\"\n",
    "conf.dropout = 0.1\n",
    "conf.train = True\n",
    "conf.lr = 1e-3\n",
    "conf.adam_beta1=0.999\n",
    "conf.adam_beta2=0.999\n",
    "conf.adam_weight_decay = 0.01\n",
    "conf.warmup_steps =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4091b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get(model_name=\"BERTLM\")(config=conf, vocab_size=len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25979e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 768])\n",
      "torch.Size([2, 768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([768, 768])\n",
      "torch.Size([768])\n",
      "torch.Size([3072, 768])\n",
      "torch.Size([3072])\n",
      "torch.Size([768, 3072])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([768])\n",
      "torch.Size([30522, 768])\n",
      "torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5787989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acdb276",
   "metadata": {},
   "source": [
    "### Loading TF - BERT FTW!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5e88327",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = torch.load(\"models/_checkpoints/wikitext2/BERTLM-latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2c5b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6eef2f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copy = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "166dc293",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model.load_state_dict(pt_model['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8027c9a",
   "metadata": {},
   "source": [
    "### Comparing with Hugging Face :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2809ad66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],\n",
       "        [-0.0117, -0.0600, -0.0323,  ..., -0.0168, -0.0401, -0.0107],\n",
       "        [-0.0198, -0.0627, -0.0326,  ..., -0.0165, -0.0420, -0.0032],\n",
       "        ...,\n",
       "        [-0.0218, -0.0556, -0.0135,  ..., -0.0043, -0.0151, -0.0249],\n",
       "        [-0.0462, -0.0565, -0.0019,  ...,  0.0157, -0.0139, -0.0095],\n",
       "        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embedding.token.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66a6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b0cf196",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEmbedding()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embedding.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ce1d4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEmbedding()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embedding.position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45ad22b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embedding.token.weight torch.Size([30522, 768])\n",
      "bert.embedding.segment.weight torch.Size([2, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.b_2 torch.Size([768])\n",
      "mask_lm.linear.weight torch.Size([30522, 768])\n",
      "mask_lm.linear.bias torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba2ec357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenEmbedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embedding.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe6aa94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512, 768)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embedding.position.pe.numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb33fc8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "           0.0000e+00,  1.0000e+00],\n",
       "         [ 8.4147e-01,  5.4030e-01,  8.2843e-01,  ...,  1.0000e+00,\n",
       "           1.0243e-04,  1.0000e+00],\n",
       "         [ 9.0930e-01, -4.1615e-01,  9.2799e-01,  ...,  1.0000e+00,\n",
       "           2.0486e-04,  1.0000e+00],\n",
       "         ...,\n",
       "         [ 6.1950e-02,  9.9808e-01,  5.3552e-01,  ...,  9.9857e-01,\n",
       "           5.2112e-02,  9.9864e-01],\n",
       "         [ 8.7333e-01,  4.8714e-01,  9.9957e-01,  ...,  9.9857e-01,\n",
       "           5.2214e-02,  9.9864e-01],\n",
       "         [ 8.8177e-01, -4.7168e-01,  5.8417e-01,  ...,  9.9856e-01,\n",
       "           5.2317e-02,  9.9863e-01]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model['model_state_dict']['bert.embedding.position.pe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7448a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embedding.token.weight torch.Size([30522, 768])\n",
      "bert.embedding.position.pe torch.Size([1, 512, 768])\n",
      "bert.embedding.segment.weight torch.Size([2, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.0.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.0.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.0.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.0.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.1.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.1.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.1.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.1.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.2.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.2.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.2.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.2.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.3.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.3.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.3.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.3.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.4.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.4.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.4.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.4.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.5.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.5.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.5.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.5.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.6.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.6.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.6.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.6.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.7.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.7.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.7.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.7.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.8.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.8.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.8.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.8.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.9.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.9.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.9.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.9.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.10.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.10.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.10.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.10.output_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.0.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.1.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.linear_layers.2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.attention.output_linear.weight torch.Size([768, 768])\n",
      "bert.transformer_blocks.11.attention.output_linear.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.weight torch.Size([3072, 768])\n",
      "bert.transformer_blocks.11.feed_forward.w_1.bias torch.Size([3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.weight torch.Size([768, 3072])\n",
      "bert.transformer_blocks.11.feed_forward.w_2.bias torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.input_sublayer.norm.b_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.a_2 torch.Size([768])\n",
      "bert.transformer_blocks.11.output_sublayer.norm.b_2 torch.Size([768])\n",
      "mask_lm.linear.weight torch.Size([30522, 768])\n",
      "mask_lm.linear.bias torch.Size([30522])\n"
     ]
    }
   ],
   "source": [
    "for key, value in pt_model['model_state_dict'].items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "acb63859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4783272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_model = deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17c5079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pt_model['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c6a2cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n"
     ]
    }
   ],
   "source": [
    "new_preds = model(data['bert_input'], data['segment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fba3da25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3364"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e9cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'bert_input':torch.tensor([[ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102,]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1932a64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(40)\n",
    "b= np.zeros(40)\n",
    "for i in range(12):\n",
    "    a[i] = [ 101, 5672, 2033, 2011, 2151, 3793, 2017, 1005, 1040, 2066, 1012,  102][i]\n",
    "    b[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ac345fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "\n",
    "new_data['bert_input'] = torch.tensor([list(a)]).int()\n",
    "new_data['segment_label'] = torch.tensor([list(b)]).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "330517f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -2.2788, -16.6172,  -7.8991,  ...,  35.9652,   1.1743,   4.1372],\n",
       "         [ -7.3481,  -6.7039,  19.1383,  ...,  10.3477,   8.1672,  -1.0014],\n",
       "         [ -1.1811,   0.5585,   0.0000,  ...,  41.6513,  -0.0000,  15.6352],\n",
       "         ...,\n",
       "         [ -4.9349,   0.6069,  26.8117,  ...,   0.0000,   0.3632,   0.3513],\n",
       "         [-31.0654,   2.4595, -22.1888,  ...,   0.0000, -11.1723,  18.1958],\n",
       "         [-12.5423, -16.8053,   0.0000,  ...,   6.1371, -10.0435,   6.6538]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert(new_data['bert_input'], new_data['segment_label'])[:, :12, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
