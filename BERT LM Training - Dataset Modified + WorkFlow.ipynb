{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi.get(\"hello\", vocab.unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "class TorchVocab(object):\n",
    "    \"\"\"Defines a vocabulary object that will be used to numericalize a field.\n",
    "    Attributes:\n",
    "        freqs: A collections.Counter object holding the frequencies of tokens\n",
    "            in the data used to build the Vocab.\n",
    "        stoi: A collections.defaultdict instance mapping token strings to\n",
    "            numerical identifiers.\n",
    "        itos: A list of token strings indexed by their numerical identifiers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
    "                 vectors=None, unk_init=None, vectors_cache=None):\n",
    "        \"\"\"Create a Vocab object from a collections.Counter.\n",
    "        Arguments:\n",
    "            counter: collections.Counter object holding the frequencies of\n",
    "                each value found in the data.\n",
    "            max_size: The maximum size of the vocabulary, or None for no\n",
    "                maximum. Default: None.\n",
    "            min_freq: The minimum frequency needed to include a token in the\n",
    "                vocabulary. Values less than 1 will be set to 1. Default: 1.\n",
    "            specials: The list of special tokens (e.g., padding or eos) that\n",
    "                will be prepended to the vocabulary in addition to an <unk>\n",
    "                token. Default: ['<pad>']\n",
    "            vectors: One of either the available pretrained vectors\n",
    "                or custom pretrained vectors (see Vocab.load_vectors);\n",
    "                or a list of aforementioned vectors\n",
    "            unk_init (callback): by default, initialize out-of-vocabulary word vectors\n",
    "                to zero vectors; can be any function that takes in a Tensor and\n",
    "                returns a Tensor of the same size. Default: torch.Tensor.zero_\n",
    "            vectors_cache: directory for cached vectors. Default: '.vector_cache'\n",
    "        \"\"\"\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        self.itos = list(specials)\n",
    "        # frequencies of special tokens are not counted when building vocabulary\n",
    "        # in frequency order\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        max_size = None if max_size is None else max_size + len(self.itos)\n",
    "\n",
    "        # sort by frequency, then alphabetically\n",
    "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        # stoi is simply a reverse dict for itos\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "        self.vectors = None\n",
    "        if vectors is not None:\n",
    "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
    "        else:\n",
    "            assert unk_init is None and vectors_cache is None\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if self.freqs != other.freqs:\n",
    "            return False\n",
    "        if self.stoi != other.stoi:\n",
    "            return False\n",
    "        if self.itos != other.itos:\n",
    "            return False\n",
    "        if self.vectors != other.vectors:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def vocab_rerank(self):\n",
    "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
    "\n",
    "    def extend(self, v, sort=False):\n",
    "        words = sorted(v.itos) if sort else v.itos\n",
    "        for w in words:\n",
    "            if w not in self.stoi:\n",
    "                self.itos.append(w)\n",
    "                self.stoi[w] = len(self.itos) - 1\n",
    "\n",
    "\n",
    "class Vocab(TorchVocab):\n",
    "    def __init__(self, counter, max_size=None, min_freq=1):\n",
    "        self.pad_index = 0\n",
    "        self.unk_index = 1\n",
    "        self.eos_index = 2\n",
    "        self.sos_index = 3\n",
    "        self.mask_index = 4\n",
    "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"],\n",
    "                         max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
    "        pass\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def save_vocab(self, vocab_path):\n",
    "        with open(vocab_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "\n",
    "# Building Vocab with text files\n",
    "class WordVocab(Vocab):\n",
    "    def __init__(self, texts, max_size=None, min_freq=1):\n",
    "        print(\"Building Vocab\")\n",
    "        counter = Counter()\n",
    "        for line in tqdm.tqdm(texts):\n",
    "            if isinstance(line, list):\n",
    "                words = line\n",
    "            else:\n",
    "                words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
    "\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
    "\n",
    "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = sentence.split()\n",
    "\n",
    "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
    "\n",
    "        if with_eos:\n",
    "            seq += [self.eos_index]  # this would be index 1\n",
    "        if with_sos:\n",
    "            seq = [self.sos_index] + seq\n",
    "\n",
    "        origin_seq_len = len(seq)\n",
    "\n",
    "        if seq_len is None:\n",
    "            pass\n",
    "        elif len(seq) <= seq_len:\n",
    "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "\n",
    "        return (seq, origin_seq_len) if with_len else seq\n",
    "\n",
    "    def from_seq(self, seq, join=False, with_pad=False):\n",
    "        words = [self.itos[idx]\n",
    "                 if idx < len(self.itos)\n",
    "                 else \"<%d>\" % idx\n",
    "                 for idx in seq\n",
    "                 if not with_pad or idx != self.pad_index]\n",
    "\n",
    "        return \" \".join(words) if join else words\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "def build():\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-c\", \"--corpus_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-o\", \"--output_path\", required=True, type=str)\n",
    "    parser.add_argument(\"-s\", \"--vocab_size\", type=int, default=None)\n",
    "    parser.add_argument(\"-e\", \"--encoding\", type=str, default=\"utf-8\")\n",
    "    parser.add_argument(\"-m\", \"--min_freq\", type=int, default=1)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.corpus_path, \"r\", encoding=args.encoding) as f:\n",
    "        vocab = WordVocab(f, max_size=args.vocab_size, min_freq=args.min_freq)\n",
    "\n",
    "    print(\"VOCAB SIZE:\", len(vocab))\n",
    "    vocab.save_vocab(args.output_path)\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, corpus_path, vocab, seq_len, encoding=\"utf-8\", corpus_lines=None, on_memory=True):\n",
    "        self.vocab = vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.on_memory = on_memory\n",
    "        self.corpus_lines = corpus_lines\n",
    "        self.corpus_path = corpus_path\n",
    "        self.encoding = encoding\n",
    "\n",
    "        with open(corpus_path, \"r\", encoding=encoding) as f:\n",
    "            if self.corpus_lines is None and not on_memory:\n",
    "                for _ in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines):\n",
    "                    self.corpus_lines += 1\n",
    "\n",
    "            if on_memory:\n",
    "                self.lines = [line[:-1].split(\"\\t\")\n",
    "                              for line in tqdm.tqdm(f, desc=\"Loading Dataset\", total=corpus_lines)]\n",
    "                self.corpus_lines = len(self.lines)\n",
    "\n",
    "        if not on_memory:\n",
    "            self.file = open(corpus_path, \"r\", encoding=encoding)\n",
    "            self.random_file = open(corpus_path, \"r\", encoding=encoding)\n",
    "\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.corpus_lines\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        t1 = self.random_sent(item)\n",
    "        print(t1)\n",
    "\n",
    "        t1_random, t1_label = self.random_word(t1)\n",
    "\n",
    "        # [CLS] tag = SOS tag, [SEP] tag = EOS tag\n",
    "        t1 = [self.vocab.sos_index] + t1_random + [self.vocab.eos_index]\n",
    "\n",
    "        t1_label = [self.vocab.pad_index] + t1_label + [self.vocab.pad_index]\n",
    "\n",
    "        segment_label = [1 for _ in range(len(t1))][:self.seq_len]\n",
    "    \n",
    "        bert_input = t1[:self.seq_len]\n",
    "        bert_label = t1_label[:self.seq_len]\n",
    "\n",
    "        padding = [self.vocab.pad_index for _ in range(self.seq_len - len(bert_input))]\n",
    "        bert_input.extend(padding), bert_label.extend(padding), segment_label.extend(padding)\n",
    "\n",
    "        output = {\"bert_input\": bert_input,\n",
    "                  \"bert_label\": bert_label,\n",
    "                  \"segment_label\": segment_label,\n",
    "                  }\n",
    "\n",
    "        return {key: torch.tensor(value) for key, value in output.items()}\n",
    "\n",
    "    def random_word(self, sentence):\n",
    "        tokens = sentence.split()        \n",
    "        n = len(tokens)\n",
    "        ix = np.random.choice(np.arange(n))\n",
    "        output_label = [0]*n\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i!=ix:\n",
    "                tokens[i] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "            else:\n",
    "                output_label[ix] = self.vocab.stoi.get(token, self.vocab.unk_index)\n",
    "                tokens[ix] = self.vocab.mask_index\n",
    "        return tokens, output_label\n",
    "\n",
    "\n",
    "    def random_sent(self, index):\n",
    "        t1 = self.get_corpus_line(index)\n",
    "        return t1\n",
    "\n",
    "    def get_corpus_line(self, item):\n",
    "        if self.on_memory:\n",
    "            return self.lines[item][0]\n",
    "\n",
    "    def get_random_line(self):\n",
    "        if self.on_memory:\n",
    "            return self.lines[random.randrange(len(self.lines))][1]\n",
    "\n",
    "        line = self.file.__next__()\n",
    "        if line is None:\n",
    "            self.file.close()\n",
    "            self.file = open(self.corpus_path, \"r\", encoding=self.encoding)\n",
    "            for _ in range(random.randint(self.corpus_lines if self.corpus_lines < 1000 else 1000)):\n",
    "                self.random_file.__next__()\n",
    "            line = self.random_file.__next__()\n",
    "        return line[:-1].split(\"\\t\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 9961it [00:00, 336651.51it/s]\n"
     ]
    }
   ],
   "source": [
    "data = BERTDataset(corpus_path=\"../wikitext-2/dataset/test_data_single_sentence.txt\",\n",
    "                  vocab=pkl.load(open(\"../wikitext-2/dataset/vocab.pkl\", \"rb\")),\n",
    "                   seq_len=20,\n",
    "                  )\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_data_loader = DataLoader(data, batch_size=1, num_workers=1)\n",
    "\n",
    "vocab=pkl.load(open(\"../wikitext-2/dataset/vocab.pkl\", \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from models.base_model import BaseModel\n",
    "from models.embedding.bert import BERTEmbedding\n",
    "from models.modules.transformer_block import TransformerBlock\n",
    "\n",
    "\n",
    "class BERT(BaseModel):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(BERT, self).__init__()\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden = 256\n",
    "        self.n_layers = 3\n",
    "        self.attn_heads = 1\n",
    "\n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = self.hidden * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(self.hidden, self.attn_heads, self.hidden * 4, 0) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def extend_parser(parser) -> argparse.ArgumentParser:\n",
    "        parser.add_argument('--hidden_features', type=int, default=768, help='# of hidden features')\n",
    "        parser.add_argument('--layers', type=int, default=12, help='# of layers in model')\n",
    "        parser.add_argument('--heads', type=int, default=12, help='# of attention heads')\n",
    "        parser.add_argument('--dropout', type=float, default=0.1, help='dropout probability')\n",
    "\n",
    "        return parser\n",
    "    \n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "\n",
    "\n",
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.mask_lm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BERT(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTLM(bert, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "EP_train:0:   0%|| 0/9961 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert <unk> is an English film , television and theatre actor .\n",
      "He had a guest @-@ starring role on the television series The Bill in 2000 .\n",
      "This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre .\n"
     ]
    }
   ],
   "source": [
    "data_iter = tqdm.tqdm(enumerate(train_data_loader),\n",
    "                      desc=\"EP_%s:%d\" % (\"train\", 0),\n",
    "                      total=len(train_data_loader),\n",
    "                      bar_format=\"{l_bar}{r_bar}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/9961 [00:05<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, data in data_iter:\n",
    "    # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "    data = {key: value for key, value in data.items()}\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_lm_output = model.forward(data[\"bert_input\"], data[\"segment_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 33237])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_lm_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   3,  672,    1,    4,   33,  364,  112,    6,  488,    9, 3053, 1639,\n",
       "             7,    2,    0,    0,    0,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]]),\n",
       " tensor([[ 0,  0,  0, 26,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0]]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"bert_input\"], data[\"segment_label\"], data[\"bert_label\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
