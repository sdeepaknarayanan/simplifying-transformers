{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5af070f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17720875",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from models.bert import BERT\n",
    "from models.modules.sublayer_connection import LayerNorm\n",
    "from models.base_model import BaseModel\n",
    "from models.embedding.bert import BERTEmbedding\n",
    "from models.modules.transformer_block import TransformerBlock\n",
    "from torch.optim import Adam\n",
    "from models.bert import ScheduledOptim\n",
    "from models.base_model import BaseModule\n",
    "from models.embedding.position import PositionalEmbedding\n",
    "from models.embedding.segment import SegmentEmbedding\n",
    "from models.embedding.token import TokenEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1fe8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(BaseModule):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = LayerNorm(embed_size, eps=1e-12)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2b42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(BaseModel):\n",
    "    def __init__(self, config, vocab_size: int, child: bool = True):\n",
    "        super(BERT, self).__init__(config)\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        self.hidden = config.hidden_features\n",
    "        self.n_layers = config.layers\n",
    "        self.attn_heads = config.heads\n",
    "        self.device = config.device\n",
    "\n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = self.hidden * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=self.hidden).to(self.conf.device)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(self.hidden, self.attn_heads, self.hidden * 4, config.dropout).to(self.conf.device)\n",
    "            for _ in range(config.layers)\n",
    "        ])\n",
    "\n",
    "        self.optimizer = Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.conf.lr,\n",
    "            betas=(self.conf.adam_beta1, self.conf.adam_beta2),\n",
    "            weight_decay=self.conf.adam_weight_decay\n",
    "        ) if config.train and not child else None\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optimizer,\n",
    "            self.hidden,\n",
    "            n_warmup_steps=self.conf.warmup_steps\n",
    "        ) if config.train and not child else None\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac196b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.layer_norm = LayerNorm(hidden)\n",
    "        self.decoder = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.act(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9ec67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLM(BaseModel):\n",
    "    def __init__(self, config, vocab_size: int):\n",
    "        super(BERTLM, self).__init__(config)\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        self.hidden = config.hidden_features\n",
    "        self.n_layers = config.layers\n",
    "        self.attn_heads = config.heads\n",
    "        self.device = config.device\n",
    "\n",
    "        self.bert = BERT(config, vocab_size, True)\n",
    "\n",
    "        self.mask_lm = MaskedLanguageModel(self.hidden, vocab_size).to(self.conf.device)\n",
    "\n",
    "\n",
    "        self.optimizer = Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.conf.lr,\n",
    "            betas=(self.conf.adam_beta1, self.conf.adam_beta2),\n",
    "            weight_decay=self.conf.adam_weight_decay\n",
    "        ) if config.train else None\n",
    "        self.optim_schedule = ScheduledOptim(\n",
    "            self.optimizer,\n",
    "            self.hidden,\n",
    "            n_warmup_steps=self.conf.warmup_steps\n",
    "        ) if config.train else None\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        x = self.bert(x, segment_info)\n",
    "        print(x.size())\n",
    "        x = self.mask_lm(x)\n",
    "        print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710d9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab = \"bert-google\"\n",
    "        self.vocab_path = \"../data/wikitext2/all.txt\"\n",
    "        self.bert_google_vocab = \"../data/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.vocab_max_size = None\n",
    "        self.vocab_min_frequency = 1\n",
    "        self.dataset = \"wikitext2\"\n",
    "        self.seq_len = 40\n",
    "        self.on_memory = True\n",
    "        self.corpus_lines = None\n",
    "        self.train_dataset = \"../data/wikitext2/test_data_single_sentence.txt\"\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1\n",
    "        self.hidden_features = 768\n",
    "        self.layers = 12\n",
    "        self.heads = 12\n",
    "        self.device = \"cpu\"\n",
    "        self.dropout = 0.1\n",
    "        self.train = True\n",
    "        self.lr = 1e-3\n",
    "        self.adam_beta1=0.999\n",
    "        self.adam_beta2=0.999\n",
    "        self.adam_weight_decay = 0.01\n",
    "        self.warmup_steps =1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbf04bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d19e97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ml = BERTLM(conf, 30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caebe296",
   "metadata": {},
   "source": [
    "### Load Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02fddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = torch.load(\"../../../../torch_dump_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "146e2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_rel_params = {}\n",
    "for name, param in pt_model.items():\n",
    "    if \"pooler\" in name or \"seq_relationship\" in name:\n",
    "        continue\n",
    "    else:\n",
    "        mlm_rel_params[name] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce07b980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.position_ids\n",
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for name in mlm_rel_params:\n",
    "    if 'embedding' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b65ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9095b46",
   "metadata": {},
   "source": [
    "### Set Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7418dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = deepcopy(bert_ml.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a49c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.position.pe'][0] = deepcopy(pt_model['bert.embeddings.position_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd6722eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.token.weight'] = deepcopy(pt_model['bert.embeddings.word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d2e6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.segment.weight'] = deepcopy(pt_model['bert.embeddings.token_type_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6a7e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.layer_norm.a_2'] = deepcopy(pt_model['bert.embeddings.LayerNorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1c7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.layer_norm.b_2'] = deepcopy(pt_model['bert.embeddings.LayerNorm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c9f8f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'attention.self.query.weight':'attention.linear_layers.0.weight',\n",
    "    'attention.self.query.bias':'attention.linear_layers.0.bias',\n",
    "    'attention.self.key.weight':'attention.linear_layers.1.weight',\n",
    "    'attention.self.key.bias':'attention.linear_layers.1.bias',\n",
    "    'attention.self.value.weight':'attention.linear_layers.2.weight',\n",
    "    'attention.self.value.bias':'attention.linear_layers.2.bias',\n",
    "    'attention.output.dense.weight':'attention.output_linear.weight',\n",
    "    'attention.output.dense.bias':'attention.output_linear.bias',\n",
    "    'attention.output.LayerNorm.weight':'input_sublayer.norm.a_2',\n",
    "    'attention.output.LayerNorm.bias': 'input_sublayer.norm.b_2',\n",
    "    'intermediate.dense.weight':'feed_forward.w_1.weight',\n",
    "    'intermediate.dense.bias':'feed_forward.w_1.bias',\n",
    "    'output.dense.weight':'feed_forward.w_2.weight',\n",
    "    'output.dense.bias':'feed_forward.w_2.bias',\n",
    "    'output.LayerNorm.weight':'output_sublayer.norm.a_2',\n",
    "    'output.LayerNorm.bias':'output_sublayer.norm.b_2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "400fe3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping = {}\n",
    "for key, value in mapping.items():\n",
    "    inv_mapping[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da04de6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_ml.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b27c192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for layer in range(12):\n",
    "    # We have 12 transformer layers, iterate through them one by one\n",
    "    for name, p_val in bert_ml.bert.transformer_blocks[layer].named_parameters():\n",
    "        to_copy = f'bert.encoder.layer.{layer}.' + inv_mapping[name]\n",
    "        param_to_copy = deepcopy(pt_model[to_copy])\n",
    "        dic[f'bert.transformer_blocks.{layer}.' + name] = param_to_copy\n",
    "        assert p_val.shape == param_to_copy.shape\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98ef9e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccaf4b",
   "metadata": {},
   "source": [
    "### Set Last Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c76006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['mask_lm.linear.weight'] = deepcopy(pt_model['cls.predictions.transform.dense.weight'])\n",
    "dic['mask_lm.linear.bias'] = deepcopy(pt_model['cls.predictions.transform.dense.bias'])\n",
    "dic['mask_lm.decoder.weight'] = deepcopy(pt_model['cls.predictions.decoder.weight'])\n",
    "dic['mask_lm.decoder.bias'] = deepcopy(pt_model['cls.predictions.decoder.bias'])\n",
    "dic['mask_lm.layer_norm.a_2'] = deepcopy(pt_model['cls.predictions.transform.LayerNorm.weight'])\n",
    "dic['mask_lm.layer_norm.b_2'] = deepcopy(pt_model['cls.predictions.transform.LayerNorm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b2437b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ml.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b024644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTLM(\n",
       "  (bert): BERT(\n",
       "    (embedding): BERTEmbedding(\n",
       "      (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
       "      (position): PositionalEmbedding()\n",
       "      (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm()\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_lm): MaskedLanguageModel(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (layer_norm): LayerNorm()\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ml.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173686a7",
   "metadata": {},
   "source": [
    "### Comparison Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b91add7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 20:49:22.847689: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "ml_model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c670821e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51b842f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Today is a cold and [MASK] day.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "96aa2215",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = ml_model(**encoded_input)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6bf3727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.ids_to_tokens[op[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4511ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input['token_type_ids']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ac2c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = ml_model(**encoded_input)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6e1c754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.ids_to_tokens[op[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcc8c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:01, 94939.50it/s] \n",
      "30522it [00:00, 473826.88it/s]\n",
      "Loading Dataset: 9961it [00:00, 601593.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets.vocabulary import BertVocab\n",
    "import datasets\n",
    "\n",
    "vocab = BertVocab(conf)\n",
    "\n",
    "# vocab.pad_index\n",
    "\n",
    "# load the dataset specified with --dataset_name & get data loaders\n",
    "train_dataset = datasets.get(dataset_name=\"wikitext2\")(config=conf, vocab=vocab)\n",
    "\n",
    "train_loader = train_dataset.get_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dbeef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "451c84df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[ 101,  103,  100, 2003, 2019, 2394, 2143, 1010, 2547, 1998, 3004, 3364,\n",
       "          1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'bert_label': tensor([[   0, 2728,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'mask_index': tensor([1])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3dd0c933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 1037, 3147, 1998,  103, 2154, 1012,  102]]), 'token_type_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "341c4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "bert_input = [[0 for i in range(10)]]\n",
    "for i in range(10):\n",
    "    bert_input[0][i] = encoded_input['input_ids'][0][i].item()\n",
    "segment_label = [[0 for i in range(10)]]\n",
    "for i in range(10):\n",
    "    segment_label[0][i] = encoded_input['token_type_ids'][0][i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693d75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['bert_input'] = torch.tensor(bert_input).int()\n",
    "new_data['segment_label'] = torch.tensor(segment_label).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc56722f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 1037, 3147, 1998,  103, 2154, 1012,  102]]), 'token_type_ids': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94575368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[ 101, 2651, 2003, 1037, 3147, 1998,  103, 2154, 1012,  102]],\n",
       "        dtype=torch.int32),\n",
       " 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=torch.int32)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "23eeca8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "torch.Size([1, 10, 30522])\n"
     ]
    }
   ],
   "source": [
    "op_our = bert_ml(new_data['bert_input'], new_data['segment_label'])[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97ea6f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n"
     ]
    }
   ],
   "source": [
    "op_our = bert_ml(data['bert_input'], data['segment_label'])[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2ce95fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[ 101,  103,  100, 2003, 2019, 2394, 2143, 1010, 2547, 1998, 3004, 3364,\n",
       "          1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'bert_label': tensor([[   0, 2728,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0]]),\n",
       " 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'mask_index': tensor([1])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a892da11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1d577736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 39])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(data['bert_input'][0].numpy()==0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86da85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ixs = np.where(data['bert_input'][0].numpy()==0)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4a8696d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 39])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ixs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2722a014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 2004 [UNK] landed a role as \" craig \" in the [MASK] \" teddy [UNK] story \" of the television series the long firm ; he starred alongside actors mark strong and derek [UNK] . [SEP] [PAD]'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(str_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.itos[op_our[mask_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67fa2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n",
      "38 1\n",
      "Mask Prediction:  james\n",
      "[MASK] [UNK] is an english film , television and theatre actor . [SEP]\n",
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n",
      "38 12\n",
      "Mask Prediction:  the\n",
      "he had a guest [UNK] starring role on the television series [MASK] bill in 2000 . [SEP]\n",
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n",
      "38 10\n",
      "Mask Prediction:  play\n",
      "this was followed by a starring role in the [MASK] [UNK] written by simon stephens , which was performed in 2001 at the royal court theatre . [SEP]\n",
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n",
      "38 8\n",
      "Mask Prediction:  television\n",
      "he had a guest role in the [MASK] series judge john [UNK] in 2002 . [SEP]\n",
      "torch.Size([1, 40, 768])\n",
      "torch.Size([1, 40, 30522])\n",
      "38 28\n",
      "Mask Prediction:  starred\n",
      "in 2004 [UNK] landed a role as \" craig \" in the episode \" teddy [UNK] story \" of the television series the long firm ; he [MASK] alongside actors mark strong and derek [UNK] . [SEP]\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "for ix, data in enumerate(train_loader):\n",
    "    ix = ix + 1\n",
    "    op_our = bert_ml(data['bert_input'], data['segment_label'])[0]\n",
    "    mask_index = data['mask_index'][0].item()\n",
    "    ixs = np.where(data['bert_input'][0].numpy()==0)[0]\n",
    "    print(n, mask_index, )\n",
    "    print(\"Mask Prediction: \", vocab.itos[op_our[mask_index].argmax().item()])\n",
    "    str_vals = []\n",
    "    for elem in data['bert_input'][0][1:-1]:\n",
    "        if elem.item() == 0:\n",
    "            break\n",
    "        str_vals.append(vocab.itos[elem.item()])\n",
    "    print(\" \".join(str_vals))\n",
    "    if ix == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30679830",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(vocab.itos[op_our[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9e232",
   "metadata": {},
   "source": [
    "## Clearly They Don't Match but WHY?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21652c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_emb_op = ml_model.bert.embeddings.forward(encoded_input['input_ids'], encoded_input['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fb947",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_emb_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_emb_op = bert_ml.bert.embedding.forward(new_data['bert_input'], new_data['segment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc93ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_emb_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3735da54",
   "metadata": {},
   "source": [
    "### Disagreement Here --> First Attn Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254024d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_data['bert_input']\n",
    "mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "bert_ml.bert.transformer_blocks[0](our_emb_op, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.encoder.layer[0].forward(ml_emb_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee74c2",
   "metadata": {},
   "source": [
    "### Investigate SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96846ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.encoder.layer[0].attention.self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.encoder.layer[0].attention.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8677682c",
   "metadata": {},
   "source": [
    "## Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050f0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_cls.key.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fa049d",
   "metadata": {},
   "source": [
    "### 0 - Q, 1 - Key, 2 - Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db690492",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ml.bert.transformer_blocks[0].attention.linear_layers[1].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in sa_cls.named_parameters():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d31288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(ml_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4006db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_q = sa_cls.transpose_for_scores(sa_cls.query(ml_emb_op))\n",
    "ml_k = sa_cls.transpose_for_scores(sa_cls.key(ml_emb_op))\n",
    "ml_v = sa_cls.transpose_for_scores(sa_cls.value(ml_emb_op))\n",
    "\n",
    "att_score = torch.matmul(ml_q, ml_k.transpose(-1, -2))\n",
    "att_score = att_score / math.sqrt(64)\n",
    "att_prob = nn.functional.softmax(att_score, dim=-1)\n",
    "\n",
    "context_layer = torch.matmul(att_prob, ml_v)\n",
    "\n",
    "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "new_context_layer_shape = context_layer.size()[:-2] + (sa_cls.all_head_size,)\n",
    "context_layer = context_layer.view(new_context_layer_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b8dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303cd71",
   "metadata": {},
   "source": [
    "## Our Network SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = bert_ml.bert.transformer_blocks[0].attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query, key, value = [l(x).view(1, -1, 12, 64).transpose(1, 2)\n",
    "                     for l, x in zip(s.linear_layers, (our_emb_op, our_emb_op, our_emb_op))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = s.attention.forward(query, key, value, mask = mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba226eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "op1 = op1[0].transpose(1, 2).contiguous().view(batch_size, -1, 12 * 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d7648",
   "metadata": {},
   "source": [
    "## Attention is the same on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242357c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.encoder.layer[0].attention.output.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7959dbf",
   "metadata": {},
   "source": [
    "### Below individual verificaiton of Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebcaff2",
   "metadata": {},
   "source": [
    "### Standard Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b2388",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ml = ml_model.bert.embeddings.word_embeddings(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ours = bert_ml.bert.embedding.token(new_data['bert_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb75c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (token_ml == token_ours).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ca8c1",
   "metadata": {},
   "source": [
    "### Segment Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd1c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ml = ml_model.bert.embeddings.token_type_embeddings(encoded_input['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_ours = bert_ml.bert.embedding.segment(new_data['segment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (seg_ml == seg_ours).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca2a3e",
   "metadata": {},
   "source": [
    "### Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ids = torch.arange(512).expand((1, -1))\n",
    "position_ids = pos_ids[:, 0 : 10 + 0]\n",
    "pos_ml = ml_model.bert.embeddings.position_embeddings(position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86be5d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ours = bert_ml.bert.embedding.position(new_data['bert_input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a697c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (pos_ml == pos_ours).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embed_ml = token_ml + seg_ml + pos_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8502a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embed_ours = token_ours + seg_ours + pos_ours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44da6e56",
   "metadata": {},
   "source": [
    "### Add Layer Norm and DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca22a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (bert_ml.bert.embedding.layer_norm.a_2 == ml_model.bert.embeddings.LayerNorm.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43658b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (bert_ml.bert.embedding.layer_norm.b_2 == ml_model.bert.embeddings.LayerNorm.bias).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.embeddings.LayerNorm(final_embed_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96253fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_ml.bert.embedding.layer_norm(final_embed_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cced6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a91b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model.bert.embeddings.forward(encoded_input['input_ids'], encoded_input['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ml.bert.embedding.forward(new_data['bert_input'], new_data['segment_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
