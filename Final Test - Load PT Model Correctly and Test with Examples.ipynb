{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13965c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b648d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from models.bert import BERTLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46af6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config():\n",
    "    def __init__(self):\n",
    "        self.vocab = \"bert-google\"\n",
    "        self.vocab_path = \"data/wikitext2/all.txt\"\n",
    "        self.bert_google_vocab = \"uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "        self.vocab_max_size = None\n",
    "        self.vocab_min_frequency = 1\n",
    "        self.dataset = \"wikitext2\"\n",
    "        self.seq_len = 40\n",
    "        self.on_memory = True\n",
    "        self.corpus_lines = None\n",
    "        self.train_dataset = \"data/wikitext2/all.txt\"\n",
    "        self.encoding = \"utf-8\"\n",
    "        self.batch_size = 1\n",
    "        self.num_workers = 1\n",
    "        self.hidden_features = 768\n",
    "        self.layers = 12\n",
    "        self.heads = 12\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.dropout = 0.1\n",
    "        self.train = True\n",
    "        self.lr = 1e-3\n",
    "        self.adam_beta1=0.999\n",
    "        self.adam_beta2=0.999\n",
    "        self.adam_weight_decay = 0.01\n",
    "        self.warmup_steps =1000\n",
    "        self.storage_directory = \"/Users/raphaelwinkler/PycharmProjects/simplifying-transformers\"\n",
    "        self.model = \"BERTLM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d35c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a89a295",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ml = BERTLM(conf, 30522)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2532968",
   "metadata": {},
   "source": [
    "### Load Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699ce1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_model = torch.load(\"torch_dump_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c526727",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_rel_params = {}\n",
    "for name, param in pt_model.items():\n",
    "    if \"pooler\" in name or \"seq_relationship\" in name:\n",
    "        continue\n",
    "    else:\n",
    "        mlm_rel_params[name] = param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a3d257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.position_ids\n",
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n"
     ]
    }
   ],
   "source": [
    "for name in mlm_rel_params:\n",
    "    if 'embedding' in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea7e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b839c4d4",
   "metadata": {},
   "source": [
    "### Set Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65ecc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = deepcopy(bert_ml.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f23e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.position.pe'][0] = deepcopy(pt_model['bert.embeddings.position_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "159db42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.token.weight'] = deepcopy(pt_model['bert.embeddings.word_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e752788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.segment.weight'] = deepcopy(pt_model['bert.embeddings.token_type_embeddings.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c9de8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.layer_norm.a_2'] = deepcopy(pt_model['bert.embeddings.LayerNorm.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b37c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['bert.embedding.layer_norm.b_2'] = deepcopy(pt_model['bert.embeddings.LayerNorm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fb85514",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'attention.self.query.weight':'attention.linear_layers.0.weight',\n",
    "    'attention.self.query.bias':'attention.linear_layers.0.bias',\n",
    "    'attention.self.key.weight':'attention.linear_layers.1.weight',\n",
    "    'attention.self.key.bias':'attention.linear_layers.1.bias',\n",
    "    'attention.self.value.weight':'attention.linear_layers.2.weight',\n",
    "    'attention.self.value.bias':'attention.linear_layers.2.bias',\n",
    "    'attention.output.dense.weight':'attention.output_linear.weight',\n",
    "    'attention.output.dense.bias':'attention.output_linear.bias',\n",
    "    'attention.output.LayerNorm.weight':'input_sublayer.norm.a_2',\n",
    "    'attention.output.LayerNorm.bias': 'input_sublayer.norm.b_2',\n",
    "    'intermediate.dense.weight':'feed_forward.w_1.weight',\n",
    "    'intermediate.dense.bias':'feed_forward.w_1.bias',\n",
    "    'output.dense.weight':'feed_forward.w_2.weight',\n",
    "    'output.dense.bias':'feed_forward.w_2.bias',\n",
    "    'output.LayerNorm.weight':'output_sublayer.norm.a_2',\n",
    "    'output.LayerNorm.bias':'output_sublayer.norm.b_2',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b46d697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_mapping = {}\n",
    "for key, value in mapping.items():\n",
    "    inv_mapping[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4684526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_ml.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00affa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for layer in range(12):\n",
    "    # We have 12 transformer layers, iterate through them one by one\n",
    "    for name, p_val in bert_ml.bert.transformer_blocks[layer].named_parameters():\n",
    "        to_copy = f'bert.encoder.layer.{layer}.' + inv_mapping[name]\n",
    "        param_to_copy = deepcopy(pt_model[to_copy])\n",
    "        dic[f'bert.transformer_blocks.{layer}.' + name] = param_to_copy\n",
    "        assert p_val.shape == param_to_copy.shape\n",
    "        cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43ec23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73a321",
   "metadata": {},
   "source": [
    "### Set Last Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c495dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['mask_lm.linear.weight'] = deepcopy(pt_model['cls.predictions.transform.dense.weight'])\n",
    "dic['mask_lm.linear.bias'] = deepcopy(pt_model['cls.predictions.transform.dense.bias'])\n",
    "dic['mask_lm.decoder.weight'] = deepcopy(pt_model['cls.predictions.decoder.weight'])\n",
    "dic['mask_lm.decoder.bias'] = deepcopy(pt_model['cls.predictions.decoder.bias'])\n",
    "dic['mask_lm.layer_norm.a_2'] = deepcopy(pt_model['cls.predictions.transform.LayerNorm.weight'])\n",
    "dic['mask_lm.layer_norm.b_2'] = deepcopy(pt_model['cls.predictions.transform.LayerNorm.bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e316f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ml.load_state_dict(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a355ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ml.save_model(running=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b5670a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERTLM(\n",
       "  (bert): BERT(\n",
       "    (embedding): BERTEmbedding(\n",
       "      (token): TokenEmbedding(30522, 768, padding_idx=0)\n",
       "      (position): PositionalEmbedding()\n",
       "      (segment): SegmentEmbedding(2, 768, padding_idx=0)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm): LayerNorm()\n",
       "    )\n",
       "    (transformer_blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): TransformerBlock(\n",
       "        (attention): MultiHeadedAttention(\n",
       "          (linear_layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (2): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attention): Attention()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feed_forward): PositionwiseFeedForward(\n",
       "          (w_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (w_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "        (input_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (output_sublayer): SublayerConnection(\n",
       "          (norm): LayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_lm): MaskedLanguageModel(\n",
       "    (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (layer_norm): LayerNorm()\n",
       "    (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    (softmax): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ml.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd003edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parent_bert import get_pretrained_berd\n",
    "\n",
    "bert_ml = get_pretrained_berd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d8123",
   "metadata": {},
   "source": [
    "### Comparison Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b21eb6",
   "metadata": {},
   "source": [
    "### Reading the Warning, it's clear it's discarding NSP layers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26d248c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "ml_model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15902a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e108f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text = \"Today is a cold and [MASK] day.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be17cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = ml_model(**encoded_input)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce7ec0",
   "metadata": {},
   "source": [
    "#### This is Hugging Face OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c36f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.ids_to_tokens[op[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f27da",
   "metadata": {},
   "source": [
    "### They use Segment Mask 0 , Just changing to 1 doesn't change OPs \n",
    "\n",
    "We need this as we use mask using 0 index!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353d527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input['token_type_ids']+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7874723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = ml_model(**encoded_input)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466c55d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.ids_to_tokens[op[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a3c74",
   "metadata": {},
   "source": [
    "### TESTING WITH OUR MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26ca1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {}\n",
    "bert_input = [[0 for i in range(10)]]\n",
    "for i in range(10):\n",
    "    bert_input[0][i] = encoded_input['input_ids'][0][i].item()\n",
    "segment_label = [[0 for i in range(10)]]\n",
    "for i in range(10):\n",
    "    segment_label[0][i] = encoded_input['token_type_ids'][0][i].item()\n",
    "\n",
    "new_data['bert_input'] = torch.tensor(bert_input).int()\n",
    "new_data['segment_label'] = torch.tensor(segment_label).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf4cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "op_our = bert_ml(new_data['bert_input'], new_data['segment_label'])[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff4e050",
   "metadata": {},
   "source": [
    "## We can see outputs are close!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9264bd84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.4448,  -6.3908,  -6.4175,  ...,  -5.8137,  -5.6503,  -3.9097],\n",
       "        [-14.2450, -14.5067, -14.0750,  ..., -12.1318, -12.2131, -13.8554],\n",
       "        [-10.1342,  -9.8525,  -9.9614,  ...,  -8.5781,  -6.2810,  -5.1799],\n",
       "        ...,\n",
       "        [-13.8870, -14.3584, -13.9323,  ..., -11.1448, -10.5272, -11.7787],\n",
       "        [-13.3764, -12.8394, -13.1806,  ..., -10.7812, -11.3668, -11.1040],\n",
       "        [-19.0271, -19.0210, -19.0633,  ..., -17.7467, -15.8703, -12.1800]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bcd0f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -6.5247,  -6.4878,  -6.5218,  ...,  -5.9450,  -5.7974,  -3.9458],\n",
       "        [-14.1594, -14.3849, -14.0029,  ..., -12.1670, -11.9847, -14.2297],\n",
       "        [ -8.6822,  -8.4283,  -8.5070,  ...,  -8.0076,  -5.6218,  -4.0874],\n",
       "        ...,\n",
       "        [-13.1808, -13.5999, -13.2546,  ..., -11.0337,  -9.9279, -10.9528],\n",
       "        [-11.6317, -11.2551, -11.4205,  ...,  -8.9649,  -9.4258, -10.4603],\n",
       "        [-14.4416, -14.7170, -14.5038,  ..., -13.1296, -11.8556, -10.9936]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op_our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f6b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Bert Vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98856it [00:00, 144997.86it/s]\n",
      "30522it [00:00, 1131455.62it/s]\n",
      "Loading Dataset: 98856it [00:00, 428726.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets.vocabulary import BertVocab\n",
    "import datasets\n",
    "\n",
    "vocab = BertVocab(conf)\n",
    "\n",
    "# vocab.pad_index\n",
    "\n",
    "# load the dataset specified with --dataset_name & get data loaders\n",
    "train_dataset = datasets.get(dataset_name=\"wikitext2\")(config=conf, vocab=vocab)\n",
    "\n",
    "train_loader = train_dataset.get_data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f53b7a9",
   "metadata": {},
   "source": [
    "## Use BERT Tokenizer and our Own to confirm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfbe3974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.ids_to_tokens[op_our[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d90818b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "today\n",
      "is\n",
      "a\n",
      "cold\n",
      "and\n",
      "rainy\n",
      "day\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(vocab.itos[op_our[i].argmax().item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5775cb9",
   "metadata": {},
   "source": [
    "## Good! Outputs match for this test case :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef16178e",
   "metadata": {},
   "source": [
    "Now let us use and check if this holds even for WikiText2 Dataset :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb3f9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "874fd38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert_input': tensor([[  101,   100,  2053,   100,  1017,  1024,   103, 11906,  1006,  2887,\n",
       "           1024,   100,  1010,  5507,  1012,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " 'bert_label': tensor([[  0,   0,   0,   0,   0,   0, 100,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]),\n",
       " 'segment_label': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'mask_index': tensor([6])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54815208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['bert_label'][0].max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aed6ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "[UNK] no [UNK] 3 : [UNK] chronicles [MASK] japanese : [UNK] , lit . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 1:  (\n",
      "True Mask for above input: (\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "[UNK] of the battlefield 3 ) , commonly referred to as [UNK] chronicles iii outside [MASK] , is a tactical role [UNK] playing video game developed by sega and [UNK] for the playstation portable . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 2:  japan\n",
      "True Mask for above input: japan\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "released in january 2011 in japan , it is the third game in the [MASK] series . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 3:  pokemon\n",
      "True Mask for above input: [UNK]\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "[UNK] the same fusion of tactical and real [UNK] time gameplay as its predecessors , the story runs parallel to the first game and follows the \" [UNK] \" , a penal military unit serving [MASK] nation of\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 4:  the\n",
      "True Mask for above input: the\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "the game began development [MASK] 2010 , carrying over a large portion of the work done on [UNK] chronicles ii . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 5:  in\n",
      "True Mask for above input: in\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "while it retained the standard features of the series , [MASK] also underwent multiple adjustments , such as making the game more [UNK] for series newcomers . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 6:  it\n",
      "True Mask for above input: it\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "character designer [UNK] [UNK] and composer [UNK] [MASK] both returned from previous entries , along with [UNK] chronicles ii director [UNK] [UNK] . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 7:  [UNK]\n",
      "True Mask for above input: [UNK]\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "a large [MASK] of writers handled the script . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 8:  number\n",
      "True Mask for above input: team\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "the game [MASK] opening theme was sung by may [UNK] . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 9:  '\n",
      "True Mask for above input: [UNK]\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "[MASK] met with positive sales in japan , and was praised by both japanese and western critics . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 10:  it\n",
      "True Mask for above input: it\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "after release , it received downloadable content , along with an expanded edition in november [MASK] that year . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 11:  of\n",
      "True Mask for above input: of\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "it was [MASK] adapted into manga and an original video animation series . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 12:  also\n",
      "True Mask for above input: also\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "due [MASK] low sales of [UNK] chronicles ii , [UNK] chronicles iii was not localized , but a fan translation compatible with the game [UNK] expanded edition was released in 2014 . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 13:  to\n",
      "True Mask for above input: to\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "[UNK] would return to the franchise [MASK] the development of [UNK] : azure revolution for the playstation 4 . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 14:  with\n",
      "True Mask for above input: with\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "as with previous [UNK] chronicles games , [UNK] chronicles iii is a tactical [MASK] [UNK] playing game where players take control of a military unit and take part in missions against enemy forces . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 15:  ,\n",
      "True Mask for above input: role\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "stories are told through comic book [UNK] like panels with animated character portraits , with characters speaking partially through voiced [MASK] bubbles and partially through [UNK] text . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 16:  speech\n",
      "True Mask for above input: speech\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "the [MASK] progresses through a series of linear missions , gradually unlocked as maps that can be freely [UNK] through and [UNK] as they are unlocked . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 17:  player\n",
      "True Mask for above input: player\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "the route to each story location on [MASK] map varies depending on an individual player [UNK] approach : when one option is selected , the other is sealed off to the player . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 18:  the\n",
      "True Mask for above input: the\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "outside missions , the player characters rest in a camp , where units can be customized and character growth occurs [MASK] [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 19:  .\n",
      "True Mask for above input: .\n",
      "\n",
      "\n",
      "\n",
      "==========================\n",
      "Input Sentence:\n",
      "\n",
      "alongside [MASK] main story missions are character [UNK] specific sub missions relating to different squad members . [SEP]\n",
      "\n",
      "\n",
      "\n",
      "Mask Prediction for input 20:  the\n",
      "True Mask for above input: the\n"
     ]
    }
   ],
   "source": [
    "ix = 0\n",
    "for ix, data in enumerate(train_loader):\n",
    "    ix = ix + 1\n",
    "    op_our = bert_ml(data['bert_input'], data['segment_label'])[0]\n",
    "    mask_index = data['mask_index'][0].item()\n",
    "    ixs = np.where(data['bert_input'][0].numpy()==0)[0]\n",
    "    n = len(ixs)\n",
    "    print(\"==========================\")\n",
    "    print(\"Input Sentence:\\n\")\n",
    "    str_vals = []\n",
    "    for elem in data['bert_input'][0][1:-1]:\n",
    "        if elem.item() == 0:\n",
    "            break\n",
    "        str_vals.append(vocab.itos[elem.item()])\n",
    "    print(\" \".join(str_vals))\n",
    "    print(\"\\n\")\n",
    "    print(f\"\\nMask Prediction for input {ix}: \", vocab.itos[op_our[mask_index].argmax().item()])\n",
    "    print(f\"True Mask for above input: {vocab.itos[data['bert_label'][0].max().item()]}\")\n",
    "    if ix == 20:\n",
    "        break\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d74ff2b1c7d4cc7edbff3bee0bfe8e6d2b36c773ab4de05dd879532ed0335fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
